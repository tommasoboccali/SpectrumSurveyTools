Your name,Which are the categories which better describe your role(s)?,Your Email (It is going to be needed if you want to get the survey results when available+ or if you agree that we can contact you to ask additional questions),"Fill if you selected ""Other"" in the previous question",Which is/are your scientific domain(s) of expertise (if applicable)?,"Add more details if relevant (and specify if you selected ""Other"")",Describe in a few words what is your activity,On behalf of whom are you submitting the survey?,Which initiative / centre? (for example: The CMS Experiment at CERN or the CINECA HPC Centre),Provide a short name for your initiative / use case / centre (for our indexing) (for example+ data analysis at ATLAS),If you want+ you can put links to pages to the activity,What is the team size (in number of collaborators) of the initiative?,Would you like to receive at your email address reports from this survey?,Would you like to be added to the SPECTRUM newsletter?,Are you (or the initiative you represent) ALSO a user of computing facilities in your scientific activity? (as a researcher+ as a programmer+ as a manager),Please select your areas of expertise+ for which you can answer technical questions:,Are you (ALSO) manager of an infrastructure? (computing centre+ a federated infrastructure+ a data centre+ ...),Provide additional details if relevant,Authentication and Authorization supported method(s) [this includes Workload and Storage management],Detail if relevant,Technical solutions supported for AA,Comment if relevant,Does your initiative need 2FA (for example due to some internal policies)?,Comment if relevant,Are your workflows / data transfers able to be executed on centres requiring 2FA?,Comment if relevant,Which AA tools do you support or make use of?,Comment if relevant,What changes do you expect in the next 5-10 years?,Typical application type(s),Please comment / specify; put a link to the application(s) if they have a URL,Typical computing access type,Please comment / specify,Granularity of job submission,Need for complex workflows?,Comment if relevant,Global CPU needs in core-hours per year (if you have the number in other units+ like node hours/year+ Teraflops/year+ ...+ please specify the unit),Global GPU needs in GPU hours per year (if you know the number in different units+ please specify),Global needs for other types of resources (for example FPGAs... please be specific),On which timescale are these needs evaluated / requested? (for example+ requests are submitted year by year+ every 5 years+ ...),Comment if relevant,Which facilities are you using?,Comment if relevant,Describe how you allocate jobs there? (links are ok),Which is (are) the most common current resource allocation pattern(s)?,Comment if relevant,Do you have a higher level Workload Management System (WMS:for example+ an initiative-specific layer handling a distributed computing infrastructure and interactive with lower level SLURM+ HTCondor or similar)?,Comment if relevant,Please explain / upload documentation / provide links describing how the high level WMS interacts with the resources and the centres,Upload if relevant,How do you discover available resources?,Comment if relevant,Access to data while the processes are executing,Comment if relevant,Do your processes require access to external (remote) services (data sources+ accounting+ monitoring+ management+. ..)?,Comment if relevant,It is difficult to grasp the complexity of workflows that can vary between observed data and simulations+ analysis and reconstruction+ etc. We ask you to identify which is the workflow which uses more processing resources+ and describe that. Please describe it briefly+ also specifying which fraction of the total processing it represents+,"Please give it a short name (e.g. ""reconstruction at CMS"") for identification purposes",Duration,Memory needed per core (if you know the answer per node+ assume ~ 100 core nodes and thus divide by 100) - please note GB = GigaByte,Local scratch disk per core (if you know the answer per node+ assume 100 cores nodes+ and divide by 100) - please note GB = GigaByte,Network I/O within the centre (if you know the answer per node+ assume 100 cores nodes+ and divide by 100) - please note MB = MegaByte,Do you have latency requirements? Please comment,Geographical Network I/O per core (if you know the answer per node+ assume 100 cores nodes+ and divide by 100). This is relevant for example for data downloading from a distant centre before/during the execution+ and data uploading at the end. Please note 1 MB = 1 MegaByte,In multinode jobs+ which is the typical # of nodes you use?,Future (5-10y) changes to the current model,upload if relevant,Total expected volume to be handled (including everything: data from instruments+ simulations+ analysis samples) - in PB (PetaBytes),Collected in how many years?,Will data volume increase linearly with time? If not+ describe your expectations (e.g. bursts+ exponential growth),Please briefly comment how the total number is composed (for example: XX PB are observed/real data+ YY PB are simulation+ ZZ PB are analysis samples ...),Where is the data produced?,Comment if relevant,Which is the typical bandwidth with which new data enters the computing infrastructure? (if collected in bursts+ average over ~ 1 month period during operation periods) - please note B = Bytes,Comment if relevant,Which is the typical bandwidth needed from your storage systems (aggregate over the infrastructure)? (if served in bursts+ average over ~ 1 month during operation periods) - please note B = Bytes,Comment if relevant,Which low level protocols are you using for data ingestion / transfer / access?,Comment if relevant,Do you use a Data Management solution to handle data ingestion / transfer / access?,Please specify name / add links to the web page / documentation,Do you need / use advanced data management features?,Comment if relevant,Do you use data movers?,Comment if relevant,Typical storage solutions,Comment if relevant,Typical data structures,Please specify which formats are you using (for example+ CSV+ XLS+ ROOT+ HDF5+ FITS+ ...),Typical file / record size(s) - please note B = Byte,Comment if relevant,Typical file / record access patterns,Comment if relevant,How many total files / records are expected in your global storage systems?,Comment if relevant,Which policies against data loss do you need?,Comment if relevant,Needs for data confidentiality / controlled access,Comment if relevant,Do you handle data respecting FAIR Principles? (see for example here),Comment if relevant,Is your data self-describing?,Comment if relevant,How do you handle metadata?,Please provide specifications / links / description,Are your data indexed / retrievable via Persistent Identifiers (PIDs)?,Please provide specifications / links / description,Please briefly explain how you think Data Management will evolve in 5-10 years,Supported base architectures,"Please specify ""Other"" or in general any comment you may have?",Supported operating systems,Which specific linux versions / specify what Other means?,Do you need a local/shared disk to be used as area of work for the process (and scratched afterwards)?,Do you need persistent storage areas accessible from the compute nodes+ typically mounted as POSIX compliant filesystems? (please note we are NOT referring here to large data repositories: mostly software areas+ calibration areas+ etc),Do you need public IPs on the compute nodes?,Granularity of resource provisioning (the quantum of computing you need to execute a workflow),Do you expect to need accelerators / special hardware?,If yes+ which is the typical fraction of the overall workflow which can be offloaded to accelerators in %? (for example+ in total fraction of operations or total fraction of the workflow time),Please add any relevant info to the previous question,How would you like to access the resources (compute nodes+ login nodes+ edge nodes)?,Please comment if relevant,How would you like to have software made available to you?,Please comment if relevant,Do you need access to virtualization software?,Please comment if relevant,Do you need root access at any point when executing workflows on the compute nodes?,Please comment if relevant,Network needs on the computing node. Does the node need IP access to/from other nodes?,Please comment if relevant,Do you need particular services to be deployed on special machines in the centre in order to operate? (edge services+ proxies+ connectors …),Comment if relevant,Do you need privileged access to them?,Do you need X11/Wayland/VNC/… graphical access to the nodes?,"About ""Computing Environment - Basic architecture""+ do you expect changes in the next 5-10 years?","About ""Computing Environment - Additional features""+ do you expect changes in the next 5-10 years?","About ""Access to resources""+ do you expect any changes in the next 5-10 years?","About ""Computing environment on the nodes""+ do you expect any changes in the next 5-10 years?",Which are the typical programming languages that you use in your initiative?,Comment if relevant,Do you rely on licensed tools (commercial compilers+ frameworks+ IDEs+ …) directly or as dependencies? List them,Please list the main software dependencies in your code,Comment if relevant+ also including additional external dependencies. If you have an external document describing the dependencies+ please include it below,External dependency documentation+ if available,CPU architectures,CPU vectorization,GPU Accelerator,Other Accelerators,Comment on all the above if relevant,What is the driving force behind the architecture / accelerators decisions?,Comment if relevant,Do support heterogeneous computing? (executing software on different platforms+ possibly including accelerators),Comment if relevant,Do you have an agreed software licensing mode?,Do you adopt FAIR for software principles? (see for example here),How is your initiative releasing software?,Comment if relevant,Programming languages: please elaborate on the evolutions you anticipate (including ones you think you will have to use and ones you would like to use),Do you anticipate using other hardware architectures than today? Which ones?,How much will your software take advantage of multi-core processors?,How much will you take advantage if vector/SIMD CPU registers?,How much will you take advantage of GPUs/FPGAs?,"Referred to the three questions above+ which would be the driving force to use ""More""? (required for performance / required for budget / hardware constraints / other)",Do you have the resources needed to evolve your software as you will need to?,If not+ what resources are you missing?,Do you think that software performance will be an issue for you in the next decade?,Who designs and maintains the software in your initiative? (analysis+ reconstruction+ simulation+ ...),Comment if relevant,Who designs and maintains the computing tools needed by your initiative? (operations+ monitoring+ accounting+ deployment+ WMS and data management+ ...),Comment if relevant,Do people enter your field with the right software skills? (for example+ at undergrad / PhD / PostDoc / Staff levels?),In the case of scientists writing (a part of) the scientific code or the computing infrastructure code+ is specific training provided on top of standard university courses?,Are they sufficient in your opinion?,On which categories could you get training?,Please specify other (software and computing related) training you received,Please specify which training you felt was missing,Do you feel researchers developing and maintaining software and managing the computing infrastructures receive appropriate recognition in your institution / initiative?,Comment if relevant,My use case #1: Description,My use case #1: Relevant links,My use case #1: Uploaded Documentation,Do you want to describe a second use case?,My use case #2: Description,My use case #2: Relevant links,My use case #2: Uploaded Documentation,Do you want to describe a third use case?,My use case #3: Description,My use case #3: Relevant links,My use case #3: Uploaded Documentation,Do you agree to be contacted in case we have additional questions?,Which type(s) of resources does your e-Infrastructure provide today?,Please provide details,Is the centre part of a larger e-infrastructure?,Please provide details,Which are the main mechanisms to get access to your infrastructure?,Please provide details,Please quantify the size of your centre Number of CPU cores:Amount,Please quantify the size of your centre Nukber of GPU boards:Amount,Please quantify the size of your centre Installed disk (PB) - not including scratch disks on the nodes:Amount,Please quantify the size of your centre Installed tape (PB):Amount,Please quantify the size of your centre Total Power used (including cooling) in MW:Amount,Please quantify the size of your centre Total surface for IT resources in squared meters:Amount,Please quantify the size of your centre (if applicable) Total number of deployable standard racks:Amount,Please provide details,Do you operate special/login nodes?,Please provide details,Do you allow users to deploy services on login / edge nodes?,Please provide details,Resource allocation methods,Please provide details,Which access patterns to the system you support?,Please provide details,How long are typical resource allocations? (project / grant length),How long can the single processes be executed for?,Please provide details,Which base system architecture is your centre supporting?,Please provide details,Which GPGPU architectures are you supporting?,Please provide details,Which is the typical # of physical cores you have per motherboard for the latest hardware procured (exclude HT),Which is the typical # of GPUs you have per motherboard for the latest hardware procured,Which is the typical memory per core you deploy+ in GB? Please note 1 GB = 1 GigaByte,Do your nodes have a local “scratch disk” in GB per core? Please note 1 GB = 1 GigaByte,Do you have system to system fast intercommunication (MPI+ …)?,Please provide details,Which type of network connection(s) are available between storage nodes? Please note Gbps = GigaBits per second,Which type of network connection(s) are available between storage and compute nodes? Please note Gbps = GigaBits per second,Please provide details to the 2 previous questions,Which type of network connection(s) are available towards other types of hardware (for example quantum hardware+ etc),Which type of network connection(s) are available to sources / destinations outside the centre (WAN)? Please note Gbps = GigaBits per second,Are (a part of) WAN network links specifically dedicated to a (set of) external centres or specific traffic (for example+ links used for LHCOPN/LHCONE via an overlay network)?,Please provide any relevant details,Which kind of IPs do your compute resources have (not considering specific login nodes or special services),Please provide any relevant details,Which routing options are available from your compute nodes? OUTGOING CONNECTIONS,Which routing options are available from your compute nodes? INCOMING CONNECTIONS,Does your centre use / support data management tools to move / access / manage data?,Please provide any relevant details,Specify the data management tool(s) your site uses and put a link to its documentation if available?,How do you manage disk-based storage?,How do you manage tape-based storage?,Please provide details to the previous two questions,Protocols to access the storage systems+ from internal hosts (for example compute nodes),Please provide details,Protocols to access the storage systems+ from external hosts (for example storage to storage geographical transfers),Please provide details,Which is the total aggregate capacity for writing to storage in your centre (summed over the storage systems if you have many) - Please note 1 GB = 1 GigaByte,Which is the total aggregate capacity for reading from storage in your centre (summed over the storage systems if you have many) - Please note 1 GB = 1 GigaByte,Which of the following features do you support?,Please provide details,Is carbon/energy footprint a relevant factor when designing / operating your centre?,Please provide details,If yes: how do you address it?,Please provide details,Have you considered deploying more efficient / different architectures in order to improve power optimization?
Stefano Bagnasco,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),bagnasco@to.infn.it,,Experimental High Energy Physics (HEP); Experimental Gravitational Waves (GW),,e-Infrastructure Board chair for the Einstein Telescope Collaboration,Representing an initiative or a centre,The Einstein Telescope Collaboration,ET,https://www.et-gw.eu/,1726,Yes,No,Yes,Workload Management; Data Management; Computing Environment,No,,,,,,,,,,,,,Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference,"GW also include ""low-latency"" data processing that is increasingly done on the distributed infrastructure. Tis is not yet started for ET.",Interactive (shell/text); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),This is what we're currently doing for Mock Data Challenges. Most of the actual workflows are yet to be defined (in the meanwhile+ they will most likely be very similar to the Virgo ones). In brief+ we still don't know much of the following.,,,,,,,,,HPC Centres; Institutional Clouds; Grid facilities,,,,,,,,,,,,,,,,,,,,,,,,In the next future (5-10y)+ the Collaboration will define and evolve its computing model+ while the experiment is being designed and built. Mock Data Challenges computing needs will ramp up+ and likely several tools will be tested as components of the final framework and environment.,,o(50PB)/yr,,Yes,,At a single experimental site; At multiple experimental sites,Depending upon final choice of ET configuration (single or dual site),,,,,,,Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),Currently using OSDF for mock data distribution. This will most likely change in the future.,,,,,,,,,,,,,,,,,,,Yes+ we are fully committed to FAIR data,,,,,,,,As mentioned+ everything is being defined. ,,,,,,,,,,,,,,,,,,,,,,,,,,We expect a fully-fledged distributed computing infrastruvtre being gradually put in place in this timescale,We expect the ability to distribute and process streaming data for low-latency analyses to be developend for the distrbuting computing infarstructure,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Einstein Telescope Computing Model,https://doi.org/10.1051/epjconf/202429504015,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stefano Bagnasco,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory),stefano.bagnasco@to.infn.it,,Experimental Gravitational Waves (GW),,Computing Coordinator fo the Virgo collaboration,Representing an initiative or a centre,The Virgo collaboration at the European Gravitational Observatory,Virgo Collaboration,https://www.virgo-gw.eu/,899,Yes,No,Yes,Workload Management; Data Management; Computing Environment; Software+ software development and software management,Yes,,,,,,,,,,,,,Near real time processing; Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference,"By ""near real time"" here we mean low-latency searches for the generation of multimessenger alerts. Note: we actually have two more or less distinct ""non-online"" computing domains+ low-latency and offline.",Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),,Single core; Multi/Many cores (on a single node),Directed Acyclic Graphs (DAGs),Some (not all) of analysis the pipelines use DAGs for job description,o(10^9) HS06 hours/year,Still unclear,,1 year,Requests are submitted yearly+ but the actual need is evaluated per-observation period o(2 years),HPC Centres; Grid facilities; Dedicated clusters (for example owned by the initiative),,About 25% of the workload is distributed to grid sites using HTCondor. Elsewhere jobs are manually managed by analysis groups.,Fair share on batch systems; Dedicated VMs or servers,Some of the low-latency searches use dedicated servers/VMs,HTCondor; glideinWMS,"There is no general central high-level WMS. Distributed ""grid"" computing accounts of ~25% of the resources and is managed by HTCondor with pilot jobs+ the rest is locally managed",A general description of the computing infratsructure can be found in https://doi.org/10.1051/epjconf/202429504047,,Static List; Rely on GRID Provisioning,,From local disks; Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution,Depending upon the site. Large owned clusters have local copies of the data+ grid jobs rely on ODFS either via the OSDF client or the CVMFS interface,Yes,"Not all the workflows need access to DQSegDB (""conditions"" database equivalent)",Offline bayesian inference event parameter estimation+ 23% for O3 data. Other analysis workflows can be *very* different,Parameter estimation,> 3 days,< 1 GB,< 1 GB,< 1 MB/s,These are offline activities+ so no specific latency requirements. Similar processes run in low-latency on dedicated resources+ with strict latency requirements (i.e.+ as fast as possible but no need for deterministic latency),None,Not Applicable,For O5 we expect an increase in the rate of events of a factor of up to 10+ proportionally increasing the needed computing power for PE. Computing power used for searches or detector characterization shoud grow less (but estimates are still vague),,5TB in at least 2 copies,14,Yes+ only during observation periods. Data rate does not grow (much) with instrument sensititvity.,Most are archived raw data. The rest is less than 10%.,At multiple experimental sites,Data is generated at Virgo+ the two LIGO sites and KAGRA. Tada are exchanged in low-latency between the collaborations+ and distributed collectively for subsequent offline analysis.,,,,,iRODS; WebDAV; XrootD,,Yes+ an in-house solution; Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),Custom tools+ plus Kafka for low-latency and (mostly) Rucio for offline.,Multiple file / object replicas; Tape long term storage; Caches to optimize access time,Data for offline processing is distributed using the OSDF cache hierarchy.,Yes (FTS),,,,Files,"Most of the data are in the specific GWF ""frame file"" format. HDF5 is also used for public data release.",,Varies wildly according to the use case+ from small 1s ,Sequential (read each file from start to end); Read one file per job; Read multiple files per job,Depending upon use case (Fourier transforms for frequency-domain analyses+ searches+ PE+...),1 million - 1 billion,,Multiple replicas on disk/tape,,Needs an AAI mechanism (see the Compute Environment later),,We are transitioning to data managed via FAIR principles,FAIR principles are being adopted for public data releases. Still much work to be done for internal data management.,"No+ it needs to be complemented with external ""metadata"" information",Most of the final analyses need no external metadata. However+ some use cases require access to the DQSegDB.,Internally developed and operated catalogue(s),,No,,Moving towards wider adoption of Rucio (and other mainstream software such as Kafka) for data management needs+ minimizing use of self-supported code.,x86_64 (Intel+ AMD+ ...); aarch64,,Windows; Generic Linux 64bit; MacOs,,Local Disk needed; No disk needed; Not needed+ but beneficial if available,Yes+ on distributed file systems (for example+ CVMFS),No+ no need,1 core; Multicore jobs on a single node (not using the while node),Nvidia GPUs; AMD GPUs,10,Depends a lot on the analysis pipeline. 10% is a wild guess across all of the use cases. The number is growing+ though.,Via a shell+ possibly via a login node; I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),Mostly. Some of the low-latency pipelines currently require direct login to be managed.,We use containers with preinstalled SW; Via CVMFS; Other (please specify),CONDA environments,No,,No,,No; Only MPI access to nodes in the same centre; Outgoing connectivity towards a selected number of subnets,MPI is used by low-latency pipelines only (AFAIK). Outgoing connectivity for sites not having a local OSDF cache instance or local managed copies of the data.,No; Yes+ we need services executing on special nodes,"""Yes"" is for sites doing low-latency processing",Yes+ we need to be able to execute certain commands (as sudoers+ for example),No,,,,,Python; C/C++,,Matlab,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),GW-specific libraries and tools (gwPy+ LALSuite+...) ,,x86_64; aarch64,,Nvidia; AMD; Intel,,,Cost/performance optimization,Lots of work being done to port pipelines on GPUs. Expect ramp up of GPU use in coming years.,We use a single / few platforms we program directly,,Other (please specify),Partially,We license our software as free and open source,The policy is to release all software under an Open Source license (any) as soon as the work for which it was developed is published. Lower-level software (libraries+ data management tools+ etc.) is all licensed as open source.,,Planned to work on non-Apple arm for computing centres.,No idea / Not applicable,No idea / Not applicable,More,Performance and budget (i.e.+ exploitation of existing resources),No,Personpower,No,,,,,,,,,,,,,The LIGO/Virgo/KAGRA computing model,https://computing.docs.ligo.org/guide/ https://doi.org/10.1051/epjconf/202429504047,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Andrea Piccinelli,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Other (please specify),apiccine@nd.edu,Convener of the CMS Workload Management group within the CMS Offline&Computing project,Experimental High Energy Physics (HEP),,Development of the CMS WM system+ management of the WM developer team,Representing an initiative or a centre,Workload Management at CMS,Workload Management at CMS,https://cms-wmcore.docs.cern.ch/,10,Yes,Yes,Yes,Workload Management,No,,,,,,,,,,,,,Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …),"The WM framework enables distributed Monte Carlo simulations and data reprocessing for the CMS experiment+ spanning more than 50 sites across the globe and harnessing the power of more than 300+000 CPUs in parallel.
Public documentation: https://github.com/dmwm/WMCore/wiki",Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),With the support of the CMS Submission Infrastructure system and the glideinWMS middleware+ the WM system split the incoming processing physics workflows+ creating grid jobs to be submitted to batch grid+ both at CERN and worldwide. It access resources both via the WLCG federation and HPC infrastructures.,Single core; Multi/Many cores (on a single node); Whole node,Job Bundles,Also Directed Acyclic Graphs,4180 kHS23/year,We are commissioning the corresponding application/workflows in the Offline&Computing project,,1 year,The numbers provided refer to 2025 approved request for resources coming from WLCG,HPC Centres; Grid facilities,,https://indico.cern.ch/event/1059494/contributions/4532543/attachments/2315285/3941127/20210923_CMS_Submission_Infrastructure_deployment.pdf,Fair share on batch systems; Opportunistic,,HTCondor; glideinWMS; Other (please specify),The CMS Collaboration has its own WM system+ as described in 6.1.2,https://indico.cern.ch/event/1059494/contributions/4532543/attachments/2315285/3941127/20210923_CMS_Submission_Infrastructure_deployment.pdf,,Rely on GRID Provisioning; Nodes / centres registering directly the computing effort (for example+ HTCondor startds joining a schedd),,From shared filesystems at the computing centre; Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution; Remote access via streaming,,Yes,,The steps involve starting with the approval and assignment of a workflow request+ making the workflow active. The system then stages the necessary input data+ evaluates it+ and sets up data transfer rules. The workflow is divided into manageable chunks for processing+ with data locations verified. Jobs proceed through open and closed running states+ depending on data availability. Once all work units are processed+ the workflow reaches completion+ and the output data is verified+ archived+ and input data rules are cleaned up.,Central production at CMS,1-3 days,2-5 GB (5 excluded),1- 20 GB,1-10 MB/s,,1-10 MB/s,Not Applicable,15kHz event rate+ minum 8-cores job (up to 16-32)+ 400 Gb/second,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Liz Sexton-Kennedy and Phat Srimanobhas,Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory),cern.sextonkennedy@gmail.com,,Experimental High Energy Physics (HEP),,I am a co-convener of the Offline software and computing program of CMS,Representing an initiative or a centre,CMS Experiment at CERN,Data processing and analysis at CMS,,300,Yes,Yes,Yes,Authentication and Authorization; Workload Management; Data Management; Software+ software development and software management,No,,Experiment/Institution SSO; Federated (for example+ via eduGain),,macaroons; X509; 2FA; tokens,,No,,No,,Indigo-IAM; CERN-SSO,,Going forward we intend to move away from user certificates and do AAI with tokens,Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …); Machine Learning/AI training/inference,,Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),,Single core; Multi/Many cores (on a single node); Whole node,Directed Acyclic Graphs (DAGs),,4180 kHS23/year,This is a future need+ so it is not clear yet.,,1 year,We participate in the WLCG resource review process,HPC Centres; Grid facilities; Dedicated clusters (for example owned by the initiative),CERN provides some CMS specific clusters,We use a pilot system provisioning system (glideInWMS),Fair share on batch systems; Opportunistic,,HTCondor; glideinWMS; Other (please specify),CMS has it’s own WM system called WMAgent,,,Rely on GRID Provisioning; Nodes / centres registering directly the computing effort (for example+ HTCondor startds joining a schedd),,From shared filesystems at the computing centre; Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution; Remote access via streaming,CMS is probably the largest user of streaming,Yes,We use all of the above services,Creation of Simulation datasets Generator+G4+Reconstruction of G4 data is the largest use for CMS ,CMS simulation,1-3 days,2-5 GB (5 excluded),1- 20 GB,1-10 MB/s,,1-10 MB/s,Not Applicable,CMS’ computing needs will increase dramatically for HL-LHC ,,We requested ~400PB of disk for 2024/25,This covers Run 3 but also the amount of Run 2 data actively being analyized,Yes+ because while we are not taking data we are generating MC.,Most of the analysis > 90% use mini or nanoAOD which are very small relative to the detector or simulated AOD data.,At a single experimental site; At multiple computing centres (for example+ in case of simulations),,10-100 GB/s,,Other (please specify),2GB/s is the minimal Tier2 specification but we need much more from Tier1s+ and T2s in the US are connected by 100GB/s links,WebDAV; XrootD,,Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),Rucio/FTS/xrootd,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...); Tape long term storage; Caches to optimize access time,,Yes (FTS),,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...); Non relational Databases (e.g. noSQL),NoSQL and SQL are used for operational meta-data and calibrations and conditions,Files; DB records,ROOT Oracle CouchDB,1 GB - 10 GB,Some of our RAW data files are larger than 10GB,Sequential (read each file from start to end); Random (read specific parts of each file); Read one file per job; Read multiple files per job,Production jobs read one file+ user analysis usually reads many,> 1 billion,,Multiple replicas on disk/tape; We rely on low level safety (RAID systems+ for example),Many of our sites use RAID or erasure coding ,Needs an AAI mechanism (see the Compute Environment later); Data embargos,We have a policy of publishing data 6/7 years after it is taken+ depending on available manpower so some of our data is public but the most recent is not,We are transitioning to data managed via FAIR principles,,Yes (no further questions asked),,Internally developed and operated catalogue(s),,Yes+ we rely on unique identifiers from the infrastructure (for example+ data identifiers from the data management systems),For the open data we use zenodo for DOIs,We will have an explosion of data during HL-LHC we will need many of our R&D effort to succeed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Go,Go is used for our web services infrastructure + CMSSW uses the others,No,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang); Commercial tools (oracle bindings+ labview+ data analysis tools….),,,x86_64; aarch64,SSE; AVX; AVX2,Nvidia; AMD; Intel,,We use Alpacka to provide GPU portability that doesn’t mean that it provides complete performance portability ,Cost/performance optimization; Decision coming from resources deployed in the centre we need to use,,We use a single / few platforms we program directly,In the future we may use a Triton service to provide inference as a service,We use Apache 2.0 licensing model,Yes,We license our software as free and open source,,I suspect we will use the same languages maybe Julia in the future,We’ll use what our computing centers provide.  At the moment that looks like CPUs and GPUs,More,More,More,We need increased performance to mitigate the increased need in HL-LHC,Yes,More advanced RSEs would be very helpful,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gina Isar,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),gina.isar@spacescience.ro,,Observational Astroparticle (not RA or GW),,I am working on ultra-high energy cosmic rays.,Individual,,data science with the Pierre Auger Observatory,,,Yes,Yes,Yes,Computing Environment; Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit; Specific linux versions (please specify),CentOS+ Alma9,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc); Yes+ on the local compute node (for example+ $HOME etc); Yes+ on distributed file systems (for example+ CVMFS),Yes,Full nodes (using all the cores); Many nodes needed for the same workflow+ with node-to-node communication,,,,Via a shell+ possibly via a login node; I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),,Preinstalled by the sysadmins; Via CVMFS,,Singularity / Apptainer,,No,,Outgoing connectivity towards a selected number of subnets; Outgoing connectivity globally,,Yes+ we need services executing on special nodes,,We need the sysadmins to start services for us+ no direct control,X11,yes,yes,yes,yes,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,,,,,Cost/performance optimization,,We use an internally designed system (for example+ macros); We use frameworks / toolkits (please specify),,,Yes,We distribute software only under dedicated agreements,,,,More,No idea / Not applicable,No idea / Not applicable,,No,budget+ manpower,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,,Some do,No specific training,No,,,"Usually physicists get to learn the needed programming languages by themself+ ""learning by doing""+ with occasional help by others already involved in similar work.  ",Yes+ but with limited career paths,,"(Very basic example) Introducing the youngsters (Bachelor+ Master+ PhD students) to scientific computing. It was a great input to take such an initiative when needed+ and moreover when possible through a (internship) practical work before starting a project.
This king of training could also be useful for researchers when introducing to a new framework+ or platform+ in order to speed up applications efficiently. ",,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clemens Lange,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),clemens.lange@cern.ch,,Experimental High Energy Physics (HEP),,Data analysis+ computing tools+ open science and research data,Individual,,data analysis at CMS,,20,Yes,No,Yes,Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,,Some do,No specific training,No,Basic programming (languages+ development tools+ ...); Distribute computing (access to Grids+ storage systems+ AAI+ ...),,Analysis workflows+ continuous integration+ software containers,No,,,,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nichol Cunningham,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),nichol.cunningham@skao.int,,Observational Radio Astronomy (RA),,Science operations for a Radio observatory+ obtaining and processing data.,Individual,,Science operations at SKAO,,,Yes,Yes,No (will skip most of the technical sections),,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,A large interferometric observing program of the Galaxy looking for spectral lines+ free-free emission+ masers on SKA-Low and SKA-Mid (I would anticipate several TB/PB sizes for data for the whole project). The datasize/storage requirements would vary with time depending on how the data is processed. The data may need to be combined+ imaged and cleaned on the SKA Regional Centre Network (This would require more storage and computing needs). Furthermore+ after the image cubes are created+ they would require custom source extraction+ spectral line fitting algorithms ran on them.,"Something along the lines of ALMA-IMF+ a LP program with the ALMA interferometer. I could imagine proposing and obtaining a complementary dataset from SKA-Low and Mid+ it would just be more data. 
https://www.almaimf.com/
",,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leonardo Cosmai,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),leonardo.cosmai@ba.infn.it,,Theoretical High Energy Physics (HEP),,Lattice Field Theories,Individual,,Nonperturbative Quantum Chromodynamics,,,Yes,Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Fortran; Perl,,,Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,AVX512,Nvidia,,,Need for more performance than simple CPUs could provide; External decision (funders+ reviewers + ..),,We use a single / few platforms we program directly,,No,No,We keep it for internal use only,,,,More,At the same level,At the same level,,Yes,,Yes+ because of a shift in scientific needs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Latchezar Betev,Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory),latchezar.betev@cern.ch,,Experimental High Energy Physics (HEP),,Management of the ALICE distributed computing infrastructure,Representing an initiative or a centre,ALICE experiment @CERN,ALICE distributed computing,https://alice-collaboration.web.cern.ch/,2000,Yes,No,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Storage+ processing and analysis of the ALICE experiment data,https://cds.cern.ch/record/2011297/files/ALICE-TDR-019.pdf,,No,,,,,,,,Yes,HPC (CPU only); HPC (CPU and GPU); HTC CPU only (Grid-style+ batch+ ...); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),https://alimonitor.cern.ch/,It is a WLCG Tier0/1/2/3,https://wlcg.web.cern.ch/,National level grants; EU/US/... level grants,https://wlcg-cric.cern.ch/core/pledge/list/,200000,2000,250,300,?,?,?,https://wlcg-cric.cern.ch/core/pledge/list/,Yes+ needed to deploy edge services,Standard WLCG VO-box https://twiki.cern.ch/twiki/bin/view/LCG/WLCGvoboxDeployment,No,,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via Cloud interfaces (k8s+ Openstack+ ...),Documentation of the various batch systems,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …),,Longer,12-24 h (24 excluded),Standard lifetime of x509 proxy delegated to the job,x84_64; aarch64,,Nvidia; AMD,,16 - 31,>4,2-4,1-10 GB,Yes (please explain),These are available in the HPCs+ but are not used.,Other (please specify),Other (please specify),All of the above+ depending on the computing centre.,,Other,LHCOPN/LHCONE+ the speed varies depending ont he computing centre,,Other,Both private and public,Connection to onsite storage nodes; Connection to a selected list of outside locations,Connection from onsite storage nodes; Other,Yes+ via internally developed tools (please specify in the next question),,https://alien.web.cern.ch/,Xrootd; dCache; EOS,Tivoli/TSM; dCache; CTA,Depending on the site,Xrootd; Other,https also.,Xrootd; Other,https also.,> 100 GB/s,> 100 GB/s,CVMFS; Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..),,Yes,Special attention given to PUE of newly designed computing facilities,Utilization of low carbon energy from the provider; Energy production in situ / in a connected site via wind+ solar+ …; Re-utilization of excess heat (for example for house /office heating); Choice of cooling solution (adiabatic vs chillers+ …); Choice of low power/performance machines at procurement; Operational choices (node shutdown when not needed+ lower clock+ job packing to maximise systems which can be shut down+ ...); We continuously monitor the PUE of the site,All of the above+ depending on the computing centre.,Yes+ we are doing it (please specify below); Yes+ we are tracing technologies to possibly do it in the near future
Oliver Keeble,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),oliver.keeble@cern.ch,,Experimental High Energy Physics (HEP),,"Provision of ""Tier 0"" storage for the LHC. Support for data-taking with solutions for storage+ archiving and data movement.",Representing an initiative or a centre,,CERN Tier0 storage,,20,,No,No (will skip most of the technical sections),,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Storage for LHC data taking.
- disk storage systems to receive data from the experiments' DAQ systems and to provide access for processing
- tape storage systems to immediately archive the incoming data
- data movement services to orchestrate the process and to distribute data globally",,,No,,,,,,,,Yes,Storage systems (disks); Storage systems (tapes),"EOS disk systems and CTA (""CERN Tape Archive"") tape systems.
NB - this form has been filled in with reference only to CERN Tier0 storage. Questions about compute+ networks and other topics will need to be handled by the appropriate experts.",It is a WLCG Tier0/1/2/3,WLCG Tier0,,Resources are pledged directly to experimental collaborations via WLCG-managed processes.,,,1EB,1EB,,,,,Yes+ needed to access the compute node,,No,,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via external services (dask+ jupyter+ ...),,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (for example via X+ VNC+ ...); Via a Cloud Middleware / Stack; Via Kubernetes; Interactive “graphical” (Web based+ for example via Jupyter notebooks+ ...),,Longer,,,,,,,,,,,,,Standard ethernet+ 100 Gbps,Standard ethernet+ 100 Gbps,,,> 100 Gbps,,,,,,,Yes+ via open source tools (please specify in the next question),"Data management by experiment frameworks (Rucio+ Dirac+ Alien) and FTS (""File Transfer Service"")",,EOS,CTA,,POSIX (for example+ NSF mount or any other fuse mount); WebDAV; Xrootd,,WebDAV; Xrootd,,> 100 GB/s,> 100 GB/s,CVMFS; Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..); SW installation tools (Module+ Conda+ Guix+ APT+ RPM+ ...),,Yes,,,,
Concezio Bozzi,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Research software engineer (writing+ testing and managing code for an initiative),concezio.bozzi@cern.ch,,Experimental High Energy Physics (HEP),,Offline computing resource coordinator for LHCb experiment,Representing an initiative or a centre,The LHCb experiment at CERN,Offline computing at LHCb,https://lhcb.web.cern.ch,60,Yes,No,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management; Training and careers,No,,Experiment/Institution SSO; Federated (for example+ via eduGain); Username / Password,,username/password; X509; tokens,,No,,No,,Indigo-IAM; Other (specify); CERN-SSO,VOMS,VOMS will be decommissioned,Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …),,Interactive (shell/text); Via some services (for example via Web services as Jupyter notebooks); Interactive (graphical - for example using X or VNC); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),,Single core; Multi/Many cores (on a single node); Whole node,No,,1e09,0,0,1 year,,HPC Centres; Institutional Clouds; Grid facilities; Dedicated clusters (for example owned by the initiative),Most of the computing comes from grid facilities. With dedicated cluster we mean the HLT farm in off-data-taking periods,Pull model via pilot jobs,Fair share on batch systems; Opportunistic; Dedicated VMs or servers,,DIRAC,,https://github.com/DIRACGrid -- documentation at https://dirac.readthedocs.io/en/latest/,,Static List; Online resource catalogue(s) maintained by collaborators,The DIRAC Configuration System maintains a static list of resources+ updated manually or with tools like CRIC and (still) BDII,No data needed; Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution; Remote access via streaming,MonteCarlo simulations do not need input data. For all other processing types (e.g. Sprucing+ Data Analysis+ Merging) we can use different techniques. On HPCs we plan to only run MonteCarlo simulations.,Other,The DIRAC pilots need access to remote services. The LHCb applications do not. We recently developed a system for running pilots outside of the worker nodes+ but this is not the standard nor preferred way of running.,The workflow using more processing resources (nearly 90% of CPU work) is MonteCarlo simulation+ consisting of several steps: event generation with e.g. Pythia and EvtGen or other generators; detector simulation through Geant4 or fast simulation techniques; event reconstruction and selection+ through a specific application. The detector simulation is the dominant contribution in terms of CPU usage. ,Montecarlo Simulation at LHCb,3h-1d,1-2 GB (2 excluded),1- 20 GB,1-10 MB/s,,< 1 MB/s,Not Applicable,We are exploiting ways to speed-up simulation with e.g. ML techniques or by using accelerators (GPUs). We foresee an increased usage of ML/AI algorithms and methods for physics analysis+ but we cannot quantify the need at this time. ,,1200,LHC Run3 lasts until 2026; LHC Run4 is expected to start in 2029 for 4-5 years. ,Data volumes will scale linearly until the mid-2030s. An upgrade is expected in Run5 (mid-2030s)+ with an increase of the data volumes of roughly a factor 7.5. ,raw data: 100PB per data-taking year; physics analysis data: 35PB per data-taking year; simulation: a few PB per year. The number in 7.1 does not include the Run5 upgrade requirements. ,At a single experimental site; At multiple computing centres (for example+ in case of simulations),multiple computing centres only in case of simulations,1-10 GB/s,the bandwidth from the experimental site is 10GB/s per live second of the LHC,1-10 GB/s,,POSIX (via mounted storage areas); WebDAV; XrootD,,Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),https://dirac.readthedocs.io/en/latest/,Multiple file / object replicas; Tape long term storage,,Yes (FTS),,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...); Relational Databases (e.g. SQL),Relational Databases only for metadata+ catalogs+ etc. ,Files,ROOT + custom for RAW data only,10 GB - 100 GB,,Sequential (read each file from start to end); Read one file per job; Read multiple files per job,,> 1 billion,,Multiple replicas on disk/tape,,Needs an AAI mechanism (see the Compute Environment later),,Other (please specify),CERN open data https://opendata.cern.ch,"No+ it needs to be complemented with external ""metadata"" information",,Internally developed and operated catalogue(s),We use the DIRAC file catalog (https://dirac.readthedocs.io/en/latest/) and and Oracle database for bookkeeping.  ,Other (please specify),We use DOIs for Open Data https://opendata.cern.ch,We expect that the current data management infrastructure will still be adequate. ,x86_64 (Intel+ AMD+ ...); aarch64,aarch64 is only available for a small subset of applications,Generic Linux 64bit,,Local Disk needed,Yes+ on distributed file systems (for example+ CVMFS),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),1 core; Multicore jobs on a single node (not using the while node); Full nodes (using all the cores),,0,,I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),,We use containers with preinstalled SW; Via CVMFS,CVMFS is preferred,Singularity / Apptainer,,No,,Outgoing connectivity towards a selected number of subnets,As written before+ we recently developed a solution for running applications without the need of network access+ but this is not the preferred solution. ,Yes+ if there in no outgoing connectivity from the compute nodes,We tested a solution with an ARC CE installed on the edge node.,No,No,Expansion of aarch64 support,No,We might be using accelerators e.g. GPUs for simulation+ but it depends on external packages (Geant4); we might be using GPUs for ML in analysis and reconstruction but no clear plans. ,No,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang); Other (please specify),We depend on LCG releases (https://ep-dep-sft.web.cern.ch/document/lcg-releases),,x86_64; aarch64,SSE; NEON; AVX; AVX2; AVX512,Nvidia; AMD; Intel,,,Cost/performance optimization; Capability to execute of more centres; External decision (funders+ reviewers + ..),,Other (please specify),We developed the Allen framework to execute the high-level software trigger application on the online farm. In principle+ this could be used also offline but no other applications exist. https://arxiv.org/pdf/1912.09161,We use a GPL3 licensing model,Partially,We license our software as free and open source,,We are watching potential languages such as Julia and Rust. ,We watch market trends for better performance / power consumption (e.g. ARM) and try to be agile in that respect. ,More,More,More,Required for performance and budget ,No,Person power+ i.e. software developers with advanced programming skills who can adapt to the evolution of the hardware landscape. ,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,we do not have enough professional software engineers ,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this; Procured from external non-research entities,We do not have enough professional software engineers ,Some do,Training provided via courses+ hackathons+ MOOC etc by the involved institutions; Training provided via courses+ hackathons+ MOOC etc by the experiment / initiative,No,Advanced programming (languages+ development tools+ ...); Distribute computing (access to Grids+ storage systems+ AAI+ ...); Efficient programming (profiling+ energy measurement+ ...); Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),,,Yes+ but with limited career paths,Good career paths at CERN but limited in number; limited career path in other institutions+ sometimes non-existent at all. ,A comprehensive description of the current picture of offline software and computing at LHCb can be found in three Technical Design Reports that are linked below.,https://cds.cern.ch/record/2319756+ https://cds.cern.ch/record/2310827+ https://cds.cern.ch/record/2717938/,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Baptiste Cecconi,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),baptiste.cecconi@obspm.fr,,Observational Radio Astronomy (RA),,Solar system radioastronomy and data management ,Representing an initiative or a centre,NenuFAR (SKA pathfinder),NenuFAR data center,https://nenufar.obs-nancay.fr/en/homepage-en/,150,Yes,No,Yes,Authentication and Authorization; Data Management; Computing Environment,Yes,we are testing EOSC AAI (Eduteams+ EGI check in...) to access our applications (first with our gitlab server),Experiment/Institution SSO; Federated (for example+ via eduGain),,tokens,,No,not sure yet how this will evolve+ but as of now+ no need for this,No,,EGI-Checkin; Other (specify),Eduteams (from GEANT),We plan to implement AAI for all services+ and to be able to delegate invitations to virtual-orgs sub-groups to proposal PIs.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,30,2,yes+ we plan to share 2 PB of new data per year. These data are reduced data (raw data is 100 x larger)  ,Only observational data (reduced data) and associated calibrators,At a single experimental site; At multiple computing centres (for example+ in case of simulations),The data is produced with the NenuFAR instrument (radio telescope)+ and is first processed in Nançay (single site+ for first step)+ and then post-processed (from reduced and pre-calibrated data) on multiple site (cloud processing / data lake),1-10 GB/s,The pre-processing in Nançay currently produces 2PB/year of data. The data should be copied in a reasonable amount of time to the external storage and computing facility,100-1000 GB/s,The size of individual observations ranges from a few 10 GB to a few 1TB+ so the data transfer is one of the limiting factor. A smart strategy of data staging with distributed computing must be put in place. ,iRODS; SCP; S3,We have been using iRODS for copying backups to a cold storage facility (but they are moving to Globus). We are also testing S3+ for our distributed storage and computing architecture. ,Yes+ a commercial solution (e.g. GlobusOnline); Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...); Other (please specify),We will to use a Globus client for data backups+ we will also test RUCIO. We are also testing Nuvla.io as a data catalogue connected to a cloud facility. ,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...); Tape long term storage; Caches to optimize access time,,No,,Cloud storage (Amazon S3+ Openstack Swift+ Oracle Cloud Storage+ Microsoft Azure storage+ ... ); Relational Databases (e.g. SQL); Non relational Databases (e.g. noSQL),,Files; DB records,FITS+ CDF (common data format+ see: https://cdf.gsfc.nasa.gov)+ MS (Measurement Set)+ HDF5,1 GB - 10 GB; 10 GB - 100 GB; 1 MB - 1 GB; > 100 GB,This depends on the type of observations+ as well as on the instrument set-up for a given observation mode.,Sequential (read each file from start to end); Random (read specific parts of each file); Read multiple files per job,This really depends of the processing pipeline... but all cases are possible.,1 thousand - 1 million,,Multiple replicas on disk/tape,Currently we have a backup on tape in another country. We plan to have multiple copies+ to increase the data accessibility from the cloud computing facility,No need (data is public); Needs an AAI mechanism (see the Compute Environment later); Data embargos,data are embargoed according to the NenuFAR science management plan (https://nenufar.obs-nancay.fr/download/nenufar-science-case/). After this embargo+ all data will be publicly shared,We are transitioning to data managed via FAIR principles,We are fully committed and we have the experience+ but it is not fully implemented for this experiment ,"Yes (no further questions asked); No+ it needs to be complemented with external ""metadata"" information",The data files in standard formats contains rich metadata+ according to their respective community usage (e.g.: FITS files with WCS metadata for astronomy observation+ FITS files with SOLARNET metadata for solar observations+  CDF with ISTP metadata for space physics observations...). Part of the metadata will be stored out of the file+ like the file provenance; and in a database to implement data discoverability (coverage+ provenance...),Use open source solutions (please specify),Astronomy metadata are shared with IVOA protocols+ using the DaCHS framework. Space and Solar physics will also be shared with the IHDEA standards (SPASE registry and metadata),Yes+ we emit PIDs internally in the experiment / initiative,Currently Observatoire de Paris can mint DOIs+ and we will start with this set up+ mapping the IVOA or IHDEA metadata to Datacite and schema.org.,We are building our infrastructure+ so this is ongoing work. ,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit,,No disk needed; Not needed+ but beneficial if available,No; Yes+ on shared disks (for example+ $HOME+ $WORK+ etc),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),Multicore jobs on a single node (not using the while node); Full nodes (using all the cores),Nvidia GPUs,0,We are only starting the GPU/accelerator action for our project+ so it is difficult to answer,Via a shell+ possibly via a login node; I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),We are implementing both access methods: shell and web interface.,We use containers with preinstalled SW,not completely decided+ but there is a need to have a registry of containers with preconfigured software (with versions)+ for radio astronomy,KVM; Singularity / Apptainer; Docker,,No,,Outgoing connectivity only to nodes in the local centre; Outgoing connectivity towards a selected number of subnets; Ingoing connectivity only from the local centre; Ingoing connectivity from a selected subnets,,Yes+ we need services executing on special nodes,We have an orchestrator which will dispatch the jobs (based on COMPSs and Lithops),We need the sysadmins to start services for us+ no direct control,VNC; X11,The use case is being developed+ so that's our goal,The use case is being developed+ so that's our goal,The use case is being developed+ so that's our goal,The use case is being developed+ so that's our goal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,NenuFAR imaging cloud processing ,,,,,,,,,,,,HPC (CPU and GPU); Cloud (CPU and GPU); Storage systems (disks),This is our plan+ since we are currently implementing.,Part of a national infrastructure (please specify); Part of a domain infrastructure (please specify); EOSC,Our infrastructure will be distributed between local (Observatoire de Paris) and national (MesoNet) insfrastructures. It will also be part of the French contribution of an international infrastructure (SRCNet). We also have a tape backup through EOSC.,Open only to some institutions' users; Other (please specify),Will be open to the science observation proposal grantees. ,,,5,,,,,not completely sized yet ,Other,not defined yet,,not defined yet,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via Cloud interfaces (k8s+ Openstack+ ...); Via external services (dask+ jupyter+ ...),still in definition phase,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (Web based+ for example via Jupyter notebooks+ ...),still in definition phase,1-3 years (3 excluded); Longer,,,,,,,,,,,,,,,,,,,,,,,,Yes+ via internally developed tools (please specify in the next question),still in development stage,testing IRODS+ RUCIO,,,,POSIX (for example+ NSF mount or any other fuse mount); S3,,S3,not fully defined yet,,,Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..),,No,We reuse existing infrastructures+ and constrained by low budget,,,
Philippe Zarka,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory),philippe.zarka@obspm.fr,,Observational Radio Astronomy (RA),,Radio search and studies of exoplanets,Individual,,EXORADIO,,10,Yes,No,Yes,Data Management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2,Yes,Observed + products,At a single experimental site,,1-10 GB/s,,1-10 GB/s,,Others (please specify),Data Center at the foot of the radiotelescope,Yes+ an in-house solution,,Multiple file / object replicas,,No,,Other (please specify),Local storage,Files,raw+ FITS+ MS,1 GB - 10 GB,,Sequential (read each file from start to end); Read multiple files per job,,1 thousand - 1 million,,Multiple replicas on disk/tape; We rely on low level safety (RAID systems+ for example),,Needs an AAI mechanism (see the Compute Environment later),,We are transitioning to data managed via FAIR principles,,Yes (no further questions asked),,Internally developed and operated catalogue(s),,No,,Included in SKA Regional Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Raw files => Reduction (~x100) => Fits files => post-processing,,read_nu_spec.pdf,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Michele Pepe,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory),pepe@mib.infn.it,,Theoretical High Energy Physics (HEP),,My group and I have been performing for more than 25 years numerical simulations of QCD on the lattice exploiting HPC systems. The codes are strongly parallel and use both MPI and MPI + OpenMPI. ,Representing an initiative or a centre,Theory Group project belonging to INFN (Istituto Nazionale Fisica Nucleare),QCDLAT,,15,Yes,Yes,Yes,Computing Environment,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit,,Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc),No+ no need,Many nodes needed for the same workflow+ with node-to-node communication,,,,Via a shell+ possibly via a login node,,Preinstalled by the sysadmins; Via Module,,No,,No,,Only MPI access to nodes in the same centre,,No,,No,No,Yes. GPU based systems are currently very powerful but+ in my field+ quite difficult to program in an efficient way; one often needs support from software engineer and that is not easy to get. I hope that future systems will be more friendly to the programmer. ,,Not much.,Probably yes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,The people belonging to the research group I coordinate+ typically perform strongly parallel Monte Carlo simulations. A typical simulation requires several thousands cores and+ in same cases+ it may scale up to order 100+000 cores. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Andreas Ipp,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),andreas.ipp@tuwien.ac.at,,Theoretical High Energy Physics (HEP),,Quark Gluon Plasma initial stages+ Lattice QCD with machine learning,Individual,,Institute for Theoretical Physics,,5,Yes,Yes,Yes,Workload Management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Access to GPUs on desktop+ on Vienna Scientific Cluster and on LEONARDO supercomputer in Italy.,https://www.vsc.ac.at/home/,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
David Schaich,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),david.schaich@liverpool.ac.uk,,Theoretical High Energy Physics (HEP),,Lattice quantum field theory,Individual,,Lattice quantum field theory,,,Yes,Yes,Yes,Data Management; Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5,5,There are generally bursts when new computing facilities come online+ followed by linear growth,The data are almost entirely lattice field configurations stochastically sampled through Monte Carlo computations+ which can be reprocessed for a variety of subsequent scientific analyses.,At multiple computing centres (for example+ in case of simulations),Mostly in the United States+ though some in Europe and India,1-10 MB/s,,1-10 MB/s,,SCP; Others (please specify),rsync,No+ using directly low level protocols,,Tape long term storage,Tape systems in the United States,No,,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...); Other (please specify),Tape for long-term storage,Files,Some HDF5+ but the vast majority of the data are in standard binary formats established by the lattice field theory community (lime+ milc+ etc.),1 GB - 10 GB; 1 kB - 1 MB; 10 GB - 100 GB; 1 MB - 1 GB,,Sequential (read each file from start to end); Read one file per job; Read multiple files per job,,1 million - 1 billion,,Multiple replicas on disk/tape,,No need (data is public),,Yes+ we are fully committed to FAIR data,,"No+ it needs to be complemented with external ""metadata"" information",,Internally developed and operated catalogue(s),Documentation and workflows included in open data releases on Zenodo,No,,There are ongoing community efforts to relaunch an international lattice data grid (ILDG) that was founded in 2002 but fell into disuse during the 2010s.  This may encourage more streamlined procedures and easier adherence to FAIR principles.  The overall amount of data I expect to continue growing roughly linearly in the long term.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,No,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); HEP specific software (ROOT+ dd4hep+...); Quantum computing tools and libraries (kiskit+ cirq+ ...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),The HEP-specific software refer to lattice field theory software packages+ for example usqcd.org/usqcd-software/,,x86_64,SSE; AVX512,,,My collaborators are heavy users of Nvidia and AMD GPUs+ but I myself am not yet involved in GPU software development,Cost/performance optimization; Capability to execute of more centres,,We use a single / few platforms we program directly,,We use a GPL3 licensing model,Yes,We license our software as free and open source,,Eventually I think I will have to become fluent with GPU programming myself,I expect to make more use of GPUs myself+ just because they're everywhere these days,At the same level,At the same level,More,The increasing reliance on GPUs by available computing facilities means we need to use them in order to make the most use out of these resources,No,Need staff with the time to work on it --- have put in proposals to fund research software engineer positions+ but these have not yet been successful,Yes+ because of a shift in scientific needs,,,,,,,,,,,,,Lattice quantum field theory calculations generally consist of two stages: Generation of ensembles of lattice field configurations through Markov-chain Monte Carlo+ followed by ideally parallelizable analyses of those field configurations to extract physics results.  This may well be familiar to you+ so I will only provide a link to some further details and summaries of scientific projects employing lattice field theory to investigate composite Higgs models+ dark matter+ symmetric mass generation+ supersymmetry and holography in addition to its traditional domain of quantum chromodynamics.,http://www.davidschaich.net/research/background.html,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yves Kemp,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),yves.kemp@desy.de,,Experimental High Energy Physics (HEP); Theoretical High Energy Physics (HEP); Observational Radio Astronomy (RA); Observational Astroparticle (not RA or GW); Other physics related domains (please specify below); Other non-physics related research domains (please specify below),Research with photons (e.g. material science+ biology+ chemistry+ ...)+ accelerator R&D+ detector R&D,Infrastructure provider+ Scientific Computing Research and Development,Representing an initiative or a centre,IDAF: Interdisciplinary Data and Analysis Facility (Central DESY scientific computing infrastructure+ operated by DESY IT department+ used by all communities mentioned above),IDAF,https://idaf.desy.de/,The core team of the IDAF is 10 FTE. The team is integrated into the central IT department+ with O(100) FTE,No (default),No,No (will skip most of the technical sections),,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Data intensive science (HEP&astroparticle experiments+ photon science experiments+ ...),,,Yes,Simulation (HEP&astroparticle+ photon science+ accelerator and detector R&D),,,No,,,,Yes,HPC (CPU and GPU); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),,It is a WLCG Tier0/1/2/3; Part of a national infrastructure (please specify); Part of a domain infrastructure (please specify); EOSC; Other (please specify),WLCG Tier-2 for ATLAS+ CMS+ LHCb ; Raw data center for Belle II ; National Analysis Facility for Germany (Tier-3); Online&Offline compute for on-site photon science user facilities (Petra-III+ FLASH+ EuXFEL); Online&offline compute for on-site HEP experiments as well as test-beam; Offline data analysis for astroparticle physics ; EOSC to come via German NFDI initiatives,Other (please specify),HEP & Astroparticle: Pledges by DESY to these communities. NAF: Pledges by DESY to the German community. Photon science: Open to users with accepted data-taking proposals. Access to in-house scientists from all communities.,75.000 (HPC + HTC),380 (HPC + HTC),~70 PB HPC+ ~150 PB HTC,~150 PB,1.3 MW (total computing center),~1.200 m2,260,,Yes+ needed to access the compute node,ssh+ remote graphical desktop environment+ Jupyter hub with notebooks integrated into batch (all based on local accounts). The Grid systems offer access via ComputeElements. HTC storage can be accessed via WAN protocols.,Other,Basically+ users cannot deploy services. However+ they can run applications acting as service under their normal user ID.,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via external services (dask+ jupyter+ ...),dask and jupyter are integrated into the batch systems,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (for example via X+ VNC+ ...); Interactive “graphical” (Web based+ for example via Jupyter notebooks+ ...); Others (please specify),Grid access to the local resources provided using Storage and Compute Elements. AAI governed by the experiments themselves.,Other,1-12 h (12 excluded); 12-24 h (24 excluded); 24-48h (48 excluded); 48-72h (72 included); > 72h,ad 12.1.9: The length of the resource allocation is determined by DESY participation in projects/ experiments+ which can span decades.,x84_64,,Nvidia,,> 64,2,2-4,10-50 GB,Yes (please explain),The Maxwell HPC system within IDAF is equipped with InfiniBand for fast MPI interprocess communication+ as well as low latency+ high bandwidth access to HPC storage,Other (please specify),Other (please specify),"The HTC storage is connected with 2x10 GE. The HPC storage is connected with EDR and HDR Infiniband.
All HTC and HPC nodes are equipped with 10 GE network. The HPC nodes are equipped with EDR or HDR100 InfiniBand in addition.",,10-100 Gbps,LHCONE,,Public IPs,IPv4 and IPv6,Connection to onsite storage nodes; Connection to other onsite compute nodes; Connection to special onsite services; Connection to a selected list of outside locations; Other,Connection from other onsite compute nodes; Connection from special onsite services,Yes+ via internally developed tools (please specify in the next question); Yes+ via commercial tools (please specify in the next question); Yes+ via open source tools (please specify in the next question),internal development: dCache ; commercial tool: GPFS ; open source tool: FTS-3 and Rucio,https://www.dcache.org/,GPFS; Lustre; dCache; NFS,dCache; CTA,,POSIX (for example+ NSF mount or any other fuse mount); WebDAV; Xrootd; Other,dcap protocol,WebDAV; Xrootd,,10-100 GB/s; > 100 GB/s,10-100 GB/s; > 100 GB/s,CVMFS; Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..); SW installation tools (Module+ Conda+ Guix+ APT+ RPM+ ...); Other (please specify),HPC: Rich centrally provided set of scientific software applications,Yes,,Utilization of low carbon energy from the provider; Re-utilization of excess heat (for example for house /office heating); Choice of cooling solution (adiabatic vs chillers+ …); Operational choices (node shutdown when not needed+ lower clock+ job packing to maximise systems which can be shut down+ ...),Further investigation in the context of EU project Research Facilities 2.0,Yes+ we are tracing technologies to possibly do it in the near future
R. Oonk,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),raymond.oonk@surf.nl,,Experimental High Energy Physics (HEP); Observational Radio Astronomy (RA); Experimental Gravitational Waves (GW); Observational Astroparticle (not RA or GW); Other physics related domains (please specify below),Observational Astronomy at other wavelengths. Experimental Astronomy.,Currently Big Data innovation and service development for Big Data science experiments.,Individual,,SURF. I work at SURF the national e-infra for The Netherlands. The answers provide should not be used as representing SURF as a whole+ neither should the answers in public publications be able to be traced back to SURF.,,600,Yes,Yes,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management,Yes,,Experiment/Institution SSO; A locally developed system (please provide details); Federated (for example+ via eduGain),,username/password; macaroons; X509; 2FA; tokens,,Yes (please comment how),internal policies for some (not yet all) systems,No,Not all workflows can run on systems requiring 2FA+ but its depends on the systems involved+ their connections and the type/goal of the workflow ,EGI-Checkin; Indigo-IAM; Other (specify); CERN-SSO,SRAM,E.g.+ Better alignment between AAI systems (AARC BPA is insufficient). Better granular support for Compute in AAI.,Model/theory calculations; Monte Carlo simulations; Near real time processing; Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference; Other (please specify),High throughput data processing with complex converging and diverging flows. ,Interactive (shell/text); Interactive (graphical - for example using X or VNC); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),Not a fan of Jupyter (or python in general)  or web services,Single core; Multi node; Multi/Many cores (on a single node); Whole node,Directed Acyclic Graphs (DAGs),Both DAGs and Job bundles,1-10M CPU core hours,Very small - GPU workloads are still in the testing phase (codes are not mature),Very small - but testing on-conventional hardware such as FPGAs TPUs+ and DPUs,1 year,,HPC Centres; Commercial Clouds; Grid facilities; Dedicated clusters (for example owned by the initiative),,Many different interfaces,Fair share on batch systems; Dedicated VMs or servers,,DIRAC; Openstack,We are experimenting with K8S. At the moment there is not yet a clear path for combining K8S and SLURM+ not is it clear that com bination of these will provide value.,SLURM cluster is built on Openstack (or bare metal) and DIRAC submits to SLURM. ,,Rely on GRID Provisioning; Nodes / centres registering directly the computing effort (for example+ HTCondor startds joining a schedd); Other,sinfo via UI,From local disks; From shared filesystems at the computing centre; Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution,,Yes,,Data processing for Radio Observations. Ca. half of all resources Im involved with.,Data processing for Radio Observations,> 3 days,>= 10 GB,> 100 GB,> 100 MB/s,,> 100 MB/s,2-16 nodes,Integration of HPC+ HTC and AI workloads into a single physical syetem supporting high performance data analytics (HPDA),,200,1,Currently we collect about 10 PB per year in additional permanently stored data year on year. In terms of total throughput its about 200 PB per year,ca 75% is real data,At a single experimental site; At a single computing centre (for example+ in case of simulations); At multiple computing centres (for example+ in case of simulations); At multiple experimental sites,,10-100 GB/s,,10-100 GB/s,,iRODS; SCP; POSIX (via mounted storage areas); GRIDFTP; S3; WebDAV; XrootD,,Yes+ an in-house solution; Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...); Tape long term storage,,Yes (please specify); Yes (FTS),,Cloud storage (Amazon S3+ Openstack Swift+ Oracle Cloud Storage+ Microsoft Azure storage+ ... ); Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...),,Files,CSV+ ROOT+ HDF5+ FITS+ MS+ NETCDF+ Xarray etc,10 GB - 100 GB,,Sequential (read each file from start to end),LOts of different patterns,> 1 billion,,Multiple replicas on disk/tape; We rely on low level safety (RAID systems+ for example),,Needs data encryption on storage,,We are transitioning to data managed via FAIR principles,,Yes (no further questions asked),Metadata could be improved,Use open source solutions (please specify),,Yes+ we emit PIDs internally in the experiment / initiative,We can emit PIDs+ but do not always do tt.,More FAIR compliant storage and DM. Increased network capabilities will show that the compute to data model is perhaps less urgent than ppl think.,x86_64 (Intel+ AMD+ ...); Other (please specify),From an innovation perspective we investigate all archs but operational support is currently limited to x86,Windows; Generic Linux 64bit; Specific linux versions (please specify),We typically support ALMA Linux and CentOS Stream (RHEL-like). On Cloud platforms a variety of OS incl. windows is offerered. ,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc); Yes+ on the local compute node (for example+ $HOME etc); Yes+ on distributed file systems (for example+ CVMFS),Yes,1 core; Multicore jobs on a single node (not using the while node); Full nodes (using all the cores); Many nodes needed for the same workflow+ with node-to-node communication,Nvidia GPUs; AMD GPUs; Quantum Computing Emulators; Xilinx FPGAs; Quantum Computing Hardware,20,There insufficient investments being done in software upgrade to easily make use of more accelerators,Via a shell+ possibly via a login node; I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),We use both,Preinstalled by the sysadmins; Via Module; We use containers with preinstalled SW; Via CVMFS,all 4 options ticked are relevant for different cases,KVM; Singularity / Apptainer; Not needed+ but we can use them,Our cloud uses KVM. Our batch systems support singularity/apptainer.,Other (please specify),If the software can not be installed in user space or via container/module then some of root can be needed (hopefully it can be handled vis sys admins).,Only MPI access to nodes in the same centre; Outgoing connectivity globally; Ingoing connectivity globally (a public node),We have a fairly open setup+ ofcourse all kinds of security measures are in place to prevent abuse,Other+ please comment,Yes+ we often run specialised services that are run on dedicated nodes/VMs and that requires dedicated network setups.,Yes+ we need to be able to execute certain commands (as sudoers+ for example),X11; Other,Potentially ARM+ RISC-V seems too far off still.,Shared (posix-like) filesystems are overloaded on large systems+ other solutions (e.g.+ object-like) are needed.,Hopefully more accelerators use and better use of existing CPU+ network and hierachical storage. This is basically a skills and capacity question for the research and research-IT community ,Not mentioned above+ but python envs (e.g.+ conda) are slowly being prevented on large systems due to overload on filesystems. More energy/cost aware computing mechanisms to help instruct users,Python; C/C++; Go; Java; Fortran; Javascript (or TS or NodeJS); R; Other (please specify),bash and similar cli languages. Its not yet clear whether there will be a critical mass for Julia and Rust.,We sometimes provide licenses for some tools e.g.+ Intel compiler+ Matlab+ FPGA programming models+ however where possible we avoid these. ,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); HEP specific software (ROOT+ dd4hep+...); Radio Astronomy specific software (CASA+ WSClean+ PRESTO…); Quantum computing tools and libraries (kiskit+ cirq+ ...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; aarch64,AVX; AVX2; AVX512,Nvidia; AMD; Intel,TPUs; FPGAs (Xilinx),We investigate many types of accelerators and non-conventional hardware ,Cost/performance optimization; Need for more performance than simple CPUs could provide; Decision coming from resources deployed in the centre we need to use,,We use frameworks / toolkits (please specify),we support heterogeneous computing where we can,Other (please specify),Partially,We license our software as free and open source,We try to be free and open source where we can,There is a mismatch in skills which is increasing. Most young people are educated (only) in python+ but we need to make progress at a deeper level where C/C++/Fortran provide a better starting point.,Potentially aarch64+ but it depends on the developments in scientific software and the associated energy cost,At the same level,No idea / Not applicable,More,energy cost and time to result,No,Many more skilled people with stable positions in research-IT and educated in C/C++/Fortran and not just python,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,WLCG,SKA+ LOFAR 2.0,,No,,,,,,,,Yes,HPC (CPU and GPU); Cloud (CPU and GPU); Quantum Emulators (classical systems emulating quantum hardware); Other (FPGAs+ TPUs+ ...) - please specify; HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),Please contact me if you want details (we are a large e-infra),It is a WLCG Tier0/1/2/3; Part of a national infrastructure (please specify); Part of a domain infrastructure (please specify); EGI Federation,We are the national infrastructure and sometimes we provide resources that are included in domain(-led) infrastructure,Pay per use; National level grants; EU/US/... level grants; Open to communities which fund the acquisition of resources,There are many services with different eligibility criteria For details contact me.,ca. 250000,ca. 1000,ca. 100 PB,ca. 200 PB,dont know,dont know,dont know,The numbers in 12.7 are constantly changing. Please do not mention these numbers in a way that they can be recognized as SURF i.e.+ please only present aggregates and not individual centers. As SURF we do not provide permission to publicly publish our answers such that these can be traced back to SURF,Yes+ needed to access the compute node; Yes+ needed to deploy edge services,,Yes+ upon verification and test from the site admins; Yes+ via containers,It depends on the service,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via Cloud interfaces (k8s+ Openstack+ ...); Via external services (dask+ jupyter+ ...),It depends on the service,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (for example via X+ VNC+ ...); Via a Cloud Middleware / Stack; Interactive “graphical” (Web based+ for example via Jupyter notebooks+ ...),It depends on the service,1-3 years (3 excluded),> 72h,It depends on the service,x84_64,,Nvidia; AMD,It depends on the service,> 64,4,4-8,> 50 GB,Yes (please explain),It depends on the service+ but one service supports MPI. For details contact me.,Standard ethernet+ 100 Gbps,Infiniband+ >100 Gbps,Different network architectures are present a.o. EVPN and HDR100,ethernet,> 100 Gbps,yes,we do lots of (high speed) network innovation investigations. Please contact me for details,Public IPs,Both public and private. It depends on the service,Connection to onsite storage nodes; Connection to other onsite compute nodes; Connection to special onsite services; Connection to a selected list of outside locations,Connection from onsite storage nodes; Connection from other onsite compute nodes; Connection from special onsite services; Connection from a selected list of outside locations,Yes+ via open source tools (please specify in the next question),,iRODS+ YODA+ dCache+ SURF Data Repository etc,GPFS; dCache; NFS; Other,dCache; Other,contact me if details are needed,POSIX (for example+ NSF mount or any other fuse mount); WebDAV; S3; Xrootd,for details contact me,WebDAV; S3; Xrootd,,> 100 GB/s,> 100 GB/s,CVMFS; Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..); SW installation tools (Module+ Conda+ Guix+ APT+ RPM+ ...),Would like to stop with conda and other python envs due to the load it generates on the filesystems,Yes,,Utilization of low carbon energy from the provider; Re-utilization of excess heat (for example for house /office heating); Choice of cooling solution (adiabatic vs chillers+ …); Choice of low power/performance machines at procurement; Operational choices (node shutdown when not needed+ lower clock+ job packing to maximise systems which can be shut down+ ...),we are in part dependent on our geo-location and the options offered by the local energy companies as well as the data center company,Yes+ we are doing it (please specify below); Yes+ we are tracing technologies to possibly do it in the near future
Vanessa HAMAR,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),hamar@cc.in2p3.fr,Computing Infrastructure Manager,Other non-physics related research domains (please specify below),,Computing Infrastructure Manager,Individual,,IN2P3 Computing Center ,,80,Yes,Yes,No (will skip most of the technical sections),,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes,HPC (CPU only); Cloud (CPU only); HTC CPU only (Grid-style+ batch+ ...); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),,It is a WLCG Tier0/1/2/3; Part of a national infrastructure (please specify); EGI Federation,,National level grants; Only open to some domains' users,,55000,72,70,270,1+5,1700,400,,Yes+ needed to access the compute node; Other,,No,,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via external services (dask+ jupyter+ ...),,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (for example via X+ VNC+ ...); Interactive “graphical” (Web based+ for example via Jupyter notebooks+ ...),,7-12 months (12 excluded),> 72h,,x84_64; aarch64,,Nvidia,,32 - 64,4,2-4,> 50 GB,Yes (please explain),Infiniband between HPC nodes,Standard ethernet+ 10 Gbps,Standard ethernet+ 10 Gbps,,,> 100 Gbps,LHCOPN 200 Gb/s  LHCONE 200Gb/s,,Public IPs,,Connection to onsite storage nodes; Connection to special onsite services; Connection to a selected list of outside locations,No connection,No+ all left to the users; Yes+ via open source tools (please specify in the next question),,,Xrootd; dCache; NFS; Other,dCache; Other,Isilon for disk based storage and HPSS for tape based storage,POSIX (for example+ NSF mount or any other fuse mount); WebDAV; S3; Xrootd; Other,,WebDAV; Xrootd; Other,BBFTP,> 100 GB/s,> 100 GB/s,CVMFS; Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..); SW installation tools (Module+ Conda+ Guix+ APT+ RPM+ ...),,Yes,,Utilization of low carbon energy from the provider; Energy production in situ / in a connected site via wind+ solar+ …; Re-utilization of excess heat (for example for house /office heating); We continuously monitor the PUE of the site,,Yes+ we are tracing technologies to possibly do it in the near future
Vanessa HAMAR,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),hamar@cc.in2p3.fr,,Other non-physics related research domains (please specify below),systems engineer,Computing Infrastructure Manager,Individual,,IN2P3 Computing Center,,,Yes,Yes,No (will skip most of the technical sections),,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HPC (CPU only); HPC (CPU and GPU); HTC CPU only (Grid-style+ batch+ ...); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),,It is a WLCG Tier0/1/2/3; Part of a national infrastructure (please specify); EGI Federation,,National level grants; Only open to some domains' users,,55000,72,70,270,1+5,1700,400,,,,No,,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via external services (dask+ jupyter+ ...),,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (for example via X+ VNC+ ...); Interactive “graphical” (Web based+ for example via Jupyter notebooks+ ...),,7-12 months (12 excluded); 1-3 years (3 excluded),> 72h,,x84_64,,Nvidia,,32 - 64,4,2-4,> 50 GB,Yes (please explain),Infiniband only for HPC cluster,Standard ethernet+ 10 Gbps,Standard ethernet+ 10 Gbps,,,> 100 Gbps,,,Public IPs,,Connection to onsite storage nodes; Connection to other onsite compute nodes; Connection to a selected list of outside locations,Connection from onsite storage nodes; Connection from other onsite compute nodes; Connection from special onsite services,,,,,,,,,,,,,,,,,,,
Daniele Cesini,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),daniele.cesini@cnaf.infn.it,,Experimental High Energy Physics (HEP),,wlcg tier1 coordinator at infn-cnaf,Representing an initiative or a centre,INFN-CNAF WLCG Tier1,INFN-CNAF WLCG Tier1,,,Yes,Yes,No (will skip most of the technical sections),,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,infrastructure manager - the data center is used by several experiments and projects belonging to several scientific domains,https://www.cnaf.infn.it/en/,,No,,,,,,,,No,HPC (CPU only); HPC (CPU and GPU); Cloud (CPU only); Cloud (CPU and GPU); Other (FPGAs+ TPUs+ ...) - please specify; HTC CPU only (Grid-style+ batch+ ...); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),https://www.cnaf.infn.it/en/,It is a WLCG Tier0/1/2/3; Part of a national infrastructure (please specify),INFN Datacloud is the national infrastructure that includes also the WLCG tier1 and 2,National level grants; Open only to some institutions' users; EU/US/... level grants; Open to communities which fund the acquisition of resources,,80000,150,100,250,1.5,,200,https://www.cnaf.infn.it/en/,Yes+ needed to access the compute node,bastion nodes for ssh/interactive  access - grid access to the HTCand storage  resources,No,used only as jump hosts,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via Cloud interfaces (k8s+ Openstack+ ...),htcondor+slur + openstack+ some clusters on k8s,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (for example via X+ VNC+ ...); Via a Cloud Middleware / Stack; Via Kubernetes; Interactive “graphical” (Web based+ for example via Jupyter notebooks+ ...),,Longer,24-48h (48 excluded),,x84_64; riscv64; aarch64,,Nvidia; AMD,,> 64,4,4-8,> 50 GB,Yes (please explain),not on the HTC farm+ only on HPC clusters,Standard ethernet+ 100 Gbps,Standard ethernet+ 100 Gbps,,,> 100 Gbps,lhcopn/one,,Public IPs,,Other,Connection from onsite storage nodes,Yes+ via open source tools (please specify in the next question),storm-webdav and xrootd,storm-webdav and xrootd,GPFS,Tivoli/TSM,,POSIX (for example+ NSF mount or any other fuse mount); WebDAV; Xrootd,,WebDAV; Xrootd,,10-100 GB/s,10-100 GB/s,CVMFS; Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..),,Yes,,Choice of cooling solution (adiabatic vs chillers+ …); We continuously monitor the PUE of the site,,Yes+ we are tracing technologies to possibly do it in the near future
Marjolein Verkouter,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Research software engineer (writing+ testing and managing code for an initiative),verkouter@jive.eu,,Observational Radio Astronomy (RA),,Head of Technical Operations and R&D department+ developing ,Representing an initiative or a centre,The Joint Institute for VLBI as a European Research Infrastructure Consortium,JIV-ERIC,https://www.jive.eu/,24 people in the institute,Yes,No,Yes,Computing Environment; Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...); Other (please specify),Working on porting computing to GPU,Generic Linux 64bit,,Local Disk needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc); Yes+ on the local compute node (for example+ $HOME etc); Yes+ on distributed file systems (for example+ CVMFS),Yes,Full nodes (using all the cores); Many nodes needed for the same workflow+ with node-to-node communication,Nvidia GPUs; AMD GPUs,81,Our process (correlation) is I/O limited usually+ few operations per bit of data; some pre-processing steps need to be done on the CPU/generic O/S level to split the data so the computation can be parallelized over the available nodes.,Via a shell+ possibly via a login node,the process needs (direct) access to (control) files ,I will install myself; We use containers with preinstalled SW,We are looking into containerizing the s/w but that's a work in progress,Singularity / Apptainer; Docker; Not needed+ but we can use them,See above - may become relevant in the (near) future,No,,Only MPI access to nodes in the same centre; Ingoing connectivity from a selected subnets; Ingoing connectivity globally (a public node),We need to get the PB-sized data into the compute centre; sometimes we correlate in real-time so data comes in from external devices directly into the compute nodes,Yes+ we need services executing on special nodes,This could be a way to relay data into the compute processes,No,VNC; X11,Yes+ we expect to move towards GPU-based computing,not really,the data providers (radio telescopes) will continue to provide the same data+ only a lot more (2x+ 3x as much as now),depending on the efficiency gain from moving to GPU (if any; currently under research+ expected outcome 2.5 years from now) it may or may impact our future needs,Python; C/C++; Fortran,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); Radio Astronomy specific software (CASA+ WSClean+ PRESTO…); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; aarch64,SSE,Nvidia,,,Cost/performance optimization; Need for more performance than simple CPUs could provide,,We use a single / few platforms we program directly,,We use a GPL3 licensing model,Partially,We license our software as free and open source,,No plans to rewrite the 250k LOC code base,(NVIDIA and/or AMD) GPU,At the same level,At the same level,More,required for performance,No,Sufficient staffing,Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,Cross-correlation of individual radio telescope signals into a data product for scientific analysis (after calibration),https://evlbi.org/evn-for-astronomers,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Andrei Tsaregorodtsev,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility); Research software engineer (writing+ testing and managing code for an initiative),atsareg@in2p3.fr,,Experimental High Energy Physics (HEP),,Workload and data management for large HEP experiments and multi-purpose distributed computing infrastructures,Representing an initiative or a centre,DIRAC Project ,DIRAC,https://dirac.egi.eu+ http://diracgrid.org,10,No (default),Yes,Yes,Workload Management,No,,,,,,,,,,,,,Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …),Managing workflows of various scientific communities (HEP+ AstroPhysics+ bioimformatics+ etc),Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),Operating a central task queue for users and dispatching user's jobs to multiple computing centers,Single core; Multi/Many cores (on a single node); Whole node,Job Bundles,Managing bulk job submissions+ complex workflows with nodes represented as jobs,~1000 core CPU years,Small fraction of above,,more than 5 years,The estimate based on the operation of the EGI Workload Manager service im the last ~10 years,HPC Centres; Institutional Clouds; Grid facilities; Dedicated clusters (for example owned by the initiative),EGI resources (grid and cloud)+ community specific resources (HPC+ clusters),Pilot based DIRAC Workload Management system,Fair share on batch systems; Opportunistic; Dedicated VMs or servers,,DIRAC,,https://dirac.readthedocs.io/en/latest/AdministratorGuide/Systems/WorkloadManagement/index.html,,Static List; Online resource catalogue(s) maintained by collaborators,Maintaining a static catalog of available resources with their access details getting information from various sources (BDII+ GocDB+ user's communications),From shared filesystems at the computing centre; Data downloaded by the job itself or its wrapper before execution,Operating a Data Management system and tools for data bookkeeping and remote access for uploading/downloading: File Catalog + data access commands,Yes,External services+ e.g. catalogs and data sources,Typical MC simulation of a large HEP experiment: bulk job submission with MC software available from a global CVMFS file system+ with application configuration as in put and resulting data files as output stored on an external data storage service.  ,MC at Pierre Auger Observatory,1-3 days,2-5 GB (5 excluded),1- 20 GB,10-100 MB/s,No latency requirement,10-100 MB/s,Not Applicable,The basic scenario stays the same. Involving more resources of HPC type with or without accelerators+ running multi-core/multi-node jobs.  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Operation of a Workload Management service - a large scale task meta-scheduler to distribute user jobs to various types of heterogenious computing resources (HPC+ grid+ cloud+ clusters+ etc) integrating ~100 computing centers and several dozens of data storage services. ,https://dirac.readthedocs.io/en/latest/index.html,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Andrea Possenti,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),andrea.possenti@inaf.it,,Observational Radio Astronomy (RA); Experimental Gravitational Waves (GW),,Set up of radio astronomical experiments; observations; interpretation of data; management and development of computing infrastructure for astrophysics,Individual,,SKA Regional Center; Meerkat Large Programs,,,Yes,No,Yes,Software+ software development and software management,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,C/C++; Fortran,,,Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Radio Astronomy specific software (CASA+ WSClean+ PRESTO…); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; riscv64,SSE,Nvidia,,,Need for more performance than simple CPUs could provide,,We use a single / few platforms we program directly,,No,Partially,We license our software as free and open source,,,,More,No idea / Not applicable,More,,No,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,,,,,,,,,,,,,Time domani analysis,,,No,,,,,,,,Yes,HPC (CPU and GPU); Storage systems (tapes),,No,,Other (please specify),Proposal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dr. Florian Rehm,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Research software engineer (writing+ testing and managing code for an initiative),florian.matthias.rehm@cern.ch,,Experimental High Energy Physics (HEP),,Working on AccGPT+ a chatbot for CERN HEP knowledge.,Representing an initiative or a centre,LLMs for CERN,AccGPT,,,Yes,No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …),,,,,Nvidia,,,Cost/performance optimization; Capability to execute of more centres; Need for more performance than simple CPUs could provide; Previous experience in the developers’ base; Decision coming from resources deployed in the centre we need to use,,We use a single / few platforms we program directly,,No,Yes,We keep it for internal use only,Might change in the future,,TPUs+ LPUs might be interesting to test,More,,More,required for performance,No,GPUs,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,LLMs,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kristen Lutz,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),kristen.lutz@surf.nl,,Experimental High Energy Physics (HEP); Theoretical High Energy Physics (HEP); Observational Radio Astronomy (RA); Experimental Gravitational Waves (GW),,Manager of data processing and cloud consultancy services,Representing an initiative or a centre,SURF,SURF IT cooperative of education and research in the Netherlands,https://www.surf.nl/en,11,Yes,Yes,No (will skip most of the technical sections),,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,No,HPC (CPU and GPU); Cloud (CPU and GPU); Quantum Emulators (classical systems emulating quantum hardware); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),,It is a WLCG Tier0/1/2/3; Part of a national infrastructure (please specify); EGI Federation,,National level grants; Open only to some institutions' users; Open to communities which fund the acquisition of resources,,HTC only: 11.737,HTC only: 34,HTC only: 22PB,HTC only: 61PB,,,,,Yes+ needed to access the compute node; Yes+ needed to provide NAT-ted access to the compute nodes; Yes+ needed to deploy edge services,,No,We provide dedicated login nodes /edge nodes as needed,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via Cloud interfaces (k8s+ Openstack+ ...),,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …),,1-3 years (3 excluded),> 72h,,x84_64,,Nvidia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serhii Afanasiev,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),afanserg22101973@gmail.com,,Experimental Nuclear Physics (NP),,photonuclear reaction,Individual,,Senior researcher at KIPT,,,Yes,Yes,Yes,Software+ software development and software management,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; Fortran,,,Python ecosystem (NumPy+ scikit-learn+ …); Simulation toolkits (geant+ fluka+ corsika+ …),,,x86_64,,Intel,,,Capability to execute of more centres; Need for more performance than simple CPUs could provide; Previous experience in the developers’ base,,We use a single / few platforms we program directly,,No,No,We keep it for internal use only,,,,Less,Less,Less,,Yes,,Yes+ because data volumes will increase,,,,,,,,,,,,,,,,No,,,,,,,,Yes,Storage systems (disks),,Part of a national infrastructure (please specify); Part of a domain infrastructure (please specify),,National level grants; Open only to some institutions' users,,,,,,,,,,Yes+ needed to access the compute node,,Yes+ via containers,,Static allocations,,Interactive “shell only”,,1-3 years (3 excluded),12-24 h (24 excluded),,x84_64,,Intel,,32 - 64,4,1-2,1-10 GB,No,,Standard ethernet+ 10 Gbps,Standard ethernet+ 10 Gbps,,,1-10 Gbps,,,Private IPs,,Connection to onsite storage nodes,Connection from onsite storage nodes,No+ all left to the users,,,NFS,Tivoli/TSM,,Xrootd,,Xrootd,,1-10 GB /s,1-10 GB /s,Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..),,No,,,,No+ since user application needs specific architectures
Volodymyr Gann,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),gann@kipt.kharkov.ua,,Theoretical Nuclear Physics (NP),,Computer calculations of radiation damages in materials ,Individual,,Usage of computer centre for big runtime calculations. ,,3 persons,Yes,Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Fortran; Other (please specify),BASIC,MCNPX+ SRIM,Simulation toolkits (geant+ fluka+ corsika+ …),,,,,,,,Need for more performance than simple CPUs could provide; Previous experience in the developers’ base; Decision coming from resources deployed in the centre we need to use,,We use a single / few platforms we program directly,,No,No,We keep it for internal use only,,Python,,More,No idea / Not applicable,No idea / Not applicable,,Yes,,Yes+ because of a shift in scientific needs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Johan Messchendorp,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),j.messchendorp@gsi.de,,Experimental Nuclear Physics (NP),,Researcher in the field of hadron physics+ group leader+ deputy head,Individual,,data analysis for HADES+ feasibility studies future experiments at FAIR,,varies depending on use case: 10-100,No (default),Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Fortran,,no,Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; aarch64,,Nvidia; AMD,,,Previous experience in the developers’ base; Decision coming from resources deployed in the centre we need to use,,We use a single / few platforms we program directly; We use an internally designed system (for example+ macros),,No,Partially,We distribute software only under dedicated agreements,,more towards python (from Fortran --> C --> C++ --> Python),,More,At the same level,More,increase in data volumes and complexity in the field,Yes,,Yes+ because data will become more complex; Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Haili Hu,Other (please specify),haili.hu@surf.nl,Technical consultant/Research supporter at an e-Infrastructure,Other physics related domains (please specify below),Not working in a specific scientific domain myself+ but supporting researchers in Astronomy+ Physics and Earth science,Technical consultancy in using e-Infrastrcture and building partnerships with research communities in Astronomy+ Physics and Earth Sciences,Representing an initiative or a centre,SURF,SURF,https://www.surf.nl/,,Yes,Yes,No (will skip most of the technical sections),,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fabio Affinito,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility); Research software engineer (writing+ testing and managing code for an initiative),f.affinito@cineca.it,,Other physics related domains (please specify below),,Managing specialist support,Individual,,specialist support at Cineca,,20,No (default),No,Yes,Computing Environment; Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit,,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),Multicore jobs on a single node (not using the while node),Nvidia GPUs; AMD GPUs,50,,Via a shell+ possibly via a login node,,I will install myself,,No,,No,,Only MPI access to nodes in the same centre,,No,,No,No,,,,,Python; C/C++; Fortran,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,AVX; AVX2,Nvidia; AMD,,,,,,,,,,,,,,,,,,,,Other (please specify),external stakeholders,Open source enthusiasts willing to collaborate,,No,Training provided via courses+ hackathons+ MOOC etc by the involved institutions; Training provided via courses+ hackathons+ MOOC etc by the experiment / initiative,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mieke Bouwhuis,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Other (please specify),bouwhuim@nikhef.nl,Computing&Software coordinator of the KM3NeT collaboration,Observational Astroparticle (not RA or GW),KM3NeT neutrino infrastructure,Organise the data processing and data management for the KM3NeT collaboration,Representing an initiative or a centre,KM3NeT collaboration,computing at KM3NeT,,about 10 active members,No (default),No,Yes,Workload Management; Data Management,No,,,,,,,,,,,,,Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …),Offline data processing includes calibration and reconstruction and is the input to all higher level analyses. Monte Carlo simulations are required for an accurate knowledge of the detector response. ,Interactive (shell/text); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),Most data processing and MC simulation is currently submitted on batch farms via an interactive node. We are transitioning to a grid solution using the Dirac middleware. ,Single core; Multi node,Directed Acyclic Graphs (DAGs),The snakemake workflow language+ which currently implements our workflows+ uses DAG.,for the current detector size about 1x10^6 core hours per year (the detector is under construction and increases in size each year+ for each new detector configuration the CPU processing needs will increase),GPUs are currently not used for MC simulations and offline data processing,no need for other types of resources,1 year,,HPC Centres; Grid facilities; Dedicated clusters (for example owned by the initiative),we are now using local clusters from different collaboration partners to do the simulation and data processing+ we are in the transition to the grid after which data processing will mainly be done by grid facilities,We login to interactive nodes of each of the local clusters from which we launch the snakemake workflow+ which submits the jobs to the worker nodes. Once the grid transition is done we will be able to use the Dirac middleware to launch the data processing and simulation.,Fair share on batch systems; Opportunistic,,DIRAC; HTCondor; Other (please specify),The different local clusters run either slurm or HTCondor. When we have concluded the grid transition we will use Dirac for most of the processing.,For the current way of data processing+ where we login to the interactive nodes+ we install the snakemake workflows which has profiles implemented for each of the systems that we use. Depending on the cluster where the snakemake workflow is launched+ the corresponding jobs are submitted+ either slurm or HTCondor.,,Rely on GRID Provisioning; Nodes / centres registering directly the computing effort (for example+ HTCondor startds joining a schedd),We have pledged resources at the different clusters+ requested each year. For the longer term+ when the grid transition is done+ we can rely on grid provisioning.,From shared filesystems at the computing centre; Data downloaded programmatically before execution+ to local/centre disks,,Yes,a gitlab service and a database,the part of the Monte Carlo simulation where the light is propagated through the detector+ this takes about 50% of the processing time (of the complete chain of Monte Carlo simulation and data processing for one data taking run),light simulation KM3NeT,> 3 days,< 1 GB,1- 20 GB,10-100 MB/s,not yet but on the longer term we aim for data processing within 1-2 days after they were recorded,10-100 MB/s,> 129 nodes,"The research infrastructure is currently under construction and is currently at about 15%  of its final size.
We are taking data while constructing the infrastructure+ the numbers given above represent the current situation (thus the 15% detector configuration).
The infrastructure will be completed in the next 5 years+ after which it will remain operational for another ~15 years.
The requirements will scale more or less linearly with the size of the infrastructure.
We are in the transition to grid processing and are setting up our workflows in Dirac.
We expect that this can go into production within the next year or 2.",,0.2 PB,8,The volume increases linearly with the number of operational modules. The infrastructure is currently under construction. Each year new modules are deployed+ but this number varies for each deployment operation. When the infrastructure is complete (it will be completed in the next ~5 years)+ the additional data volume per year will be steady+ and we will continue data taking for about 15 years. ,"raw data: ~25%
processed & simulation data: ~64%
calibration data: ~ 3%
run & environment data: ~8%",At multiple experimental sites,we have two experimental sites,< 1 MB/s,,Other (please specify),I am not sure what is required+ the storage bandwidth is (now) not a limiting factor,iRODS; POSIX (via mounted storage areas); WebDAV; XrootD,,No+ using directly low level protocols; Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),At the moment we are using the low level protocols+ we are setting up a Rucio instance+ once this is in production we will do the majority of the ingest+ transfer and access via Rucio,Multiple file / object replicas; Tape long term storage; Caches to optimize access time,,Yes (FTS),this is in the case of Rucio+ currently in the setup phase,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...); Relational Databases (e.g. SQL),,Files,ROOT mainly,1 GB - 10 GB,,Sequential (read each file from start to end); Read one file per job,,1 thousand - 1 million,,Multiple replicas on disk/tape,,Needs an AAI mechanism (see the Compute Environment later); Data embargos,a proper AAI solution like Indigo-IAM is not in place yet,We are transitioning to data managed via FAIR principles,,"No+ it needs to be complemented with external ""metadata"" information",,Internally developed and operated catalogue(s),currently we have an internal implementation of the handling of the metadata in file headers+ when the Rucio data management system is in place we can make use of the Rucio metadata handling capabilities,No,we don't have this,The research infrastructure is currently under construction and will be completed in the next ~5 years. The data  volume will increase more or less linearly with the number of operational modules. At the moment we are at about 15% of the size of the final infrastructure. The numbers in 7.1 represent the situation for a 15% infrastructure size. When the infrastructure is complete+ we expect to require 2.5 PB storage per year. Data taking will continue for ~15 years after completion. The setup of the Rucio data management system is currently in the setup phase+ we expect to commission it in the next 1-2 years.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adrien Matta,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Research software engineer (writing+ testing and managing code for an initiative),matta@lpccaen.in2p3.fr,,Experimental Nuclear Physics (NP),,researcher in nuclear physics,Individual,,nptool,nptool.in2p3.fr,20,Yes,Yes,Yes,Data Management; Computing Environment; Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5,5,yes,0.25 raw data+ 0.1 analysed data+ 0.15 simulated data,At multiple computing centres (for example+ in case of simulations); At multiple experimental sites,,1-10 GB/s,,1-10 GB/s,,iRODS; SCP,,No+ using directly low level protocols,,Multiple file / object replicas; Caches to optimize access time,,No,,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...),,Files,root,1 GB - 10 GB,,Read one file per job; Read multiple files per job,,1 thousand - 1 million,,Multiple replicas on disk; We rely on low level safety (RAID systems+ for example),,Needs an AAI mechanism (see the Compute Environment later); Data embargos,,We are transitioning to data managed via FAIR principles,,"No+ it needs to be complemented with external ""metadata"" information",,Internally developed and operated catalogue(s),,Yes+ we rely on unique identifiers from the infrastructure (for example+ data identifiers from the data management systems),,the biggest challenge is the management of Auxilliary Data (logs+ machine state+...) in a consistent way,x86_64 (Intel+ AMD+ ...); Other (please specify); aarch64,,Generic Linux 64bit; MacOs,,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc),Yes,Multicore jobs on a single node (not using the while node); Many nodes needed for the same workflow+ with node-to-node communication,Nvidia GPUs; AMD GPUs; Intel GPUs,25,,Via a shell+ possibly via a login node,,I will install myself; We use containers with preinstalled SW,,Singularity / Apptainer; Docker,,No,,No,,No,,No,No,no,no,no,no,C/C++,,no,Simulation toolkits (geant+ fluka+ corsika+ …); HEP specific software (ROOT+ dd4hep+...),,,x86_64; aarch64,,,,,Need for more performance than simple CPUs could provide,,We use frameworks / toolkits (please specify),,No,Yes,We put our software in the public domain,,c++ is well suited for the task and we don't foresee a change,yes+ GPU and tensor in général where applicable,More,More,More,required for performance,No,man power,Yes+ because data will become more complex; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software,,Some do,No specific training,No,Distribute computing (access to Grids+ storage systems+ AAI+ ...); Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),trainning on git and std usage,,No,Software development is completely absent from the nuclear physics community career path and some time seen as negative.,We target small and short live nuclear physics collaboration with a strong need for modularity in both analysis and simulation. With quickly changing setup and coupling of different DAQ systems+ the use of a single tool across multiple collaboration is solving a large part of the issues.,https://iopscience.iop.org/article/10.1088/1742-6596/2438/1/012006,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lene Kristian Bryngemark,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),lene_kristian.bryngemark@fysik.lu.se,,Experimental High Energy Physics (HEP),,sw development+ instrutmentation design+ computing coordination+ using/managing/developing the distributed computing system ,Individual,,sw&computing for LDMX,,40,Yes,Yes,Yes,Software+ software development and software management,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,,,,,,,,Other (please specify),we use docker containers (ubuntu-based) to allow use across computing centers (with the detour over apptainer) as well as user's laptops  ,No,No,We put our software in the public domain,,we rely on ROOT for persistency (a choice considering the decades-long horizon; we don't think the LHC will abandon ROOT any time soon)+ and will stick with c++ for speed anyway. the strong tendency to use python in the developer community may shift; it's used for interactions with our core software in many ways but may well shift if developer preferences shift+ as they do in a community where most code is written by junior collaborators+ who are always in flux.,can't be specific here+ we will go along with the evolution of HPCs (and laptops+ case in point is apple silicon),At the same level,No idea / Not applicable,More,again+ this will mostly be hardware driven. we will surely try and make the most of whatever hardware is offered. but this is in turn a response to a need to get the best performance which translates to faster results -- which in the end is a great way to deal with budget constraints. ,No,"i was tempted to answer Yes+ since our software will evolve the way it can with the resources we have+ but+ 

for an optimal evolution we would need long-term stability in the developer base. job security for developers of infrastructure+ rather than a developer community which targets ""analysis"" code and moves on. we do see and benefit from a lot of great skill and talent for the underlying infrastructure (dealing with e.g. the containerized model+ or CI/CD workflow+ or skillful performance optimization) among grad students+ but these people will eventually have to move on to find a permanent job+ and will likely not be able to stay connected to a small experiment with a limited number of institutes to choose from. ",Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data volumes will increase,,,,,,,,,,,,,"One use case of specific interest is our small distributed computing system. It's based on ARC+ and integrates four HPCs from partner institutions in the collaboration with minimal extensions to our specific needs. We built it to enable access to data across institutes as we are not a CERN experiment and don't have any means of central authentication+ no VOMS or similar. We have had to rely on generating our own certificates+ and access to the batch queues through one central user with a robot certificate+ to which our users in turn are granted access through actual personal trust. There are however issues with local/national policy around who uses computing resources+ accesses data+ etc. There is also no way of securing local support at HPCs beyond good will+ as there is no guarantor organization for a MoU+ authentication+ etc+ and of course no real funding allocated for it+ for an experiment which is still in its design phase and doesn't count as an infrastructure (interestingly+ computing _is_ the only infrastructure for an experiment which so far only exists digitally). We are increasingly seeing refusal from new potential sites to either a) allocate the small effort needed to integrate into our system b) grant our researchers admin privileges on some machine to do the work c) host a machine that is paid for by research funds and managed entirely by the local researcher in question. All of this quoting security concerns which often come with a hint of international relations. 

I think there is a case for a virtual guarantor organisation playing a similar role as a host lab in the sense CERN does+ but in a truly international fashion. There is also a clash in how local HPCs think about their user base (the local research community+ with storage needs for a few years for a specific project) and how we operate (global community+ large (simulation) data sets that will outlive one or several storage disks and should ideally be backed up).",,,No,,,,,,,,Yes,HPC (CPU only); HTC CPU only (Grid-style+ batch+ ...),i consider the distributed computing system an infrastructure even as it is not a computing center in itself,No,,Open only to some institutions' users,,2000,,800TB,,,,,stating the accessible resources to us within the collaborating clusters+ which are naturally much larger,No,,No,,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Other,scheduling+ brokering+ and job management overall by ARC,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …),,,,,,,,,,,,,,,,,,,,,,,,,,Yes+ via open source tools (please specify in the next question),,ARC (and using the GridFTP protocol+ considering moving to Xrootd),GPFS; NFS,,,POSIX (for example+ NSF mount or any other fuse mount),,Other,GridFTP,,,Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..),,,,,,
Mohanraj,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),mohanraj.senniappan@lnu.se,,Observational Astroparticle (not RA or GW),,Detector simulation and Analysis,Individual,,None,,,Yes,Yes,No (will skip most of the technical sections),,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Andrea Idini,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),andrea.idini@matfys.lth.se,,Theoretical Nuclear Physics (NP),,Inventing many-body models for nuclear structure and reactions and implement new computational codes to predict properties and processes of nuclei.,Individual,,Models for nuclear structure and reactions,,3,Yes,Yes,Yes,Computing Environment; Software+ software development and software management,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit,,Local Disk needed; Shared disk areas needed,Yes+ on the local compute node (for example+ $HOME etc); Yes+ on distributed file systems (for example+ CVMFS),No+ no need,1 core; Multicore jobs on a single node (not using the while node); Full nodes (using all the cores); Many nodes needed for the same workflow+ with node-to-node communication,,0,In the future+ accelerator with high memory availability might be able to offload a lot of tasks that are now only CPU-based. However development is needed and the lack of resources for such software developments is a big obstacle to more efficient accelerator-based computing.,Via a shell+ possibly via a login node,,I will install myself; Via Module,,Not needed+ but we can use them,,No,,Only MPI access to nodes in the same centre,,No,,No,No; X11,No,No,More use of X11 or graphical UI,The use of accelerator with high memory might render a lot of multi-node calculations to single-node.,Python; C/C++; Fortran; Other (please specify),Python mostly as wrapper and launcher+ always single node and not HPC. Also Bash for the same purpose.,No,Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),Fortran Libraries and Compilers for multi node: ScaLAPACK+ etc...,,x86_64,SSE; AVX; AVX2; AVX512,,,,Capability to execute of more centres; Need for more performance than simple CPUs could provide,,We use an internally designed system (for example+ macros),,No,Partially,We put our software in the public domain,We keep some for internal use which is very difficult to release with the limited resources at our disposal+ while publish some of the most useful and user--friendly and self--contained parts.,C+ C+++ and Fortran will remain the workhorses for development of High Performance Computing codes. Python consolidated its role as a wrapper. I don't foresee major changes.,"Accelerators would be interesting to implement+ but the investment in terms of development time it's right now hardly worth the effort. The availability of software developers+ ""brainware""+ to help refactor codes with the intent of porting some algorithms to GPU without compromising on reliability and ease-of-use would be the true game changer.",At the same level,At the same level,More,More memory per processing unit.,No,"Accelerators would be interesting to implement+ but the investment in terms of development time it's right now hardly worth the effort. The availability of software developers+ ""brainware""+ to help refactor codes with the intent of porting some algorithms to GPU without compromising on reliability and ease-of-use would be the true game changer.",Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,"My use case consist of simulating the interaction between quantum particles with a sophisticated formalism. This is a purely number-crunching activity+ very few data are going into the code+ and most of the data coming out are as calculations checkpoints or for debugging and cross check.

Our is the almost ideal simulation operation+ where request goes in+ an enormous amount of computing comes out to have a simple answer to a question consisting of few numbers+ i.e. the energy spectrum and transition rates between different energy levels.

Most of the simulations rooted in many-body theory are like these+ which are important for a huge amount of systems+ from atoms+ to molecules+ to atomic nuclei and slabs of materials.

Within this context+ we don't care much for data analysis or other libraries or languages. At the end of the days ours are codes that generates large matrices that represent particles and then diagonise such matrices in a ""smart"" (i.e. physics-motivated way) that will tell us about the properties of some physical objects and the causes of such properties.

Languages for this are by construction C(++) and Fortran+ because most other linear algebra routines of other languages are just wrappers for Lapack and boost. Computing FLOPs (and relative vectorisation and efficiency) are kings. Memory is necessary+ depending on the formalism and the case under consideration+ due to the size of the arrays could require 1GB per computing core or even more.

Right now accelerators are starting to have enough memory-per-computing-core that could tackle many cases and formalisms. The main things holding us back to implement in accelerators right now is personell that understand the computational and software-engineering aspects of a scientific project and that can help with the development and implementation on new computing platform. Whether this implementation is actually beneficial is currently unknown for our specific case.",https://journals.aps.org/prc/abstract/10.1103/PhysRevC.106.014314,,No,,,,,,,,Yes,HPC (CPU only); HPC (CPU and GPU); Storage systems (disks),It's a local cluster of workstations and computing racks that shares a storage space for the around 50 users our the division of mathematical-physics.,No,,Only open to some domains' users,,1000,10,0.1,,,,,It's composed of around 50 workstations+ and about two racks of computing nodes and one rack of service (login+ storage) severs.,Yes+ needed to access the compute node,,No,,Static allocations,,,,Longer,> 72h,,x84_64,,Nvidia,,32 - 64,1,2-4,> 50 GB,Yes (please explain),,Standard ethernet+ 10 Gbps,Standard ethernet+ 10 Gbps,,,,,,,,,,No+ all left to the users,,,NFS,,,POSIX (for example+ NSF mount or any other fuse mount),,,,,,,,No,,,,Other (please specify)
Greg Bailey,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),greg.bailey@ukaea.uk,,Theoretical Nuclear Physics (NP),,Development of theories amd modeeles to improve nuclear data libraries.,Representing an initiative or a centre,UKAEA,Nuclear Data,,,No (default),No,Yes,Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Fortran,,,Python ecosystem (NumPy+ scikit-learn+ …); Simulation toolkits (geant+ fluka+ corsika+ …); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,,,,,Cost/performance optimization; External decision (funders+ reviewers + ..),,We use a single / few platforms we program directly,,No,Partially,We distribute software only under dedicated agreements,Both Licenced and open source distribution,,,More,No idea / Not applicable,More,,No,,Yes+ because of a shift in scientific needs,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software,,Some do,Training provided via courses+ hackathons+ MOOC etc by the involved institutions,Yes,Basic programming (languages+ development tools+ ...); Advanced programming (languages+ development tools+ ...),,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jon Butterworth,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),j.buterworth@ucl.ac.uk,,Experimental High Energy Physics (HEP),,data analysis and interpretation at ATLAS,Individual,,data analysis and interpretation at ATLAS,,,Yes,Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,gitlab,Python ecosystem (NumPy+ scikit-learn+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..),,,x86_64,,,,,External decision (funders+ reviewers + ..),,We use a single / few platforms we program directly,,We use a GPL3 licensing model,Yes,We put our software in the public domain,,,probably GPUs,More,More,More,performance and availablity,No,people,Yes+ because of a shift in scientific needs,,,,,,,,,,,,,,,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Achim Stahl,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory),stahl@physik.rwth-aachen.de,,Experimental High Energy Physics (HEP); Experimental Gravitational Waves (GW); Observational Astroparticle (not RA or GW),,Coordinator of the Einstein Telescope in Germany+ chair of the ET computing board.,Individual,,Einstein Telescope,,,No (default),No,Yes,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),pippo@pluto.com,,Theoretical Gravitational Waves (GW),,dsd,Individual,,dsds,,,,,No (will skip most of the technical sections),,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Europass_CV__4_.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
THULLIEZ,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),loic.thulliez@cea.fr,,Experimental Nuclear Physics (NP),,I am conducting nuclear and neutrino physics experiments. I develops simulation codes (Geant4+ TOUCANS+ FIFRELIN+ etc)+ run the simulations and performs data analysis.,Individual,,code developer at Geant4+ code developer and data analysis for CANS and CRAB,,,Yes,No,Yes,Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); Simulation toolkits (geant+ fluka+ corsika+ …); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,,,,,,,We use a single / few platforms we program directly; We use frameworks / toolkits (please specify),,No,Yes,We put our software in the public domain,We keep it for internal use only AND We put our software in the public domain,,,More,No idea / Not applicable,No idea / Not applicable,,No,Manpower,No,,,,,Yes,No specific training,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Héctor Alvarez Pol,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),hector.alvarez@usc.es,,Experimental Nuclear Physics (NP),,Nuclear Physics simulations and data analysis+ direct reactions+ quasifree reactions+ fission+ calorimetry+ ....,Individual,,Data analysis and simulation at R3B,,100,Yes,Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,C/C++,,No,Simulation toolkits (geant+ fluka+ corsika+ …); HEP specific software (ROOT+ dd4hep+...); Quantum computing tools and libraries (kiskit+ cirq+ ...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),FAIRSOFT-FAIRROOT,,x86_64,,,,,,,We use frameworks / toolkits (please specify),,We use LGPL licensing model,Yes,We license our software as free and open source,,C+++ python,,At the same level,No idea / Not applicable,More,,Yes,,Yes+ because data will become more complex; Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gergely Gábor Barnaföldi,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility); Research software engineer (writing+ testing and managing code for an initiative),barnafoldi.gergely@wigner.hun-ren.hu,,Experimental High Energy Physics (HEP); Theoretical High Energy Physics (HEP); Experimental Nuclear Physics (NP); Theoretical Nuclear Physics (NP),,Theory+ phenomenology+ and experimental group leading in high-energy nuclear physics,Individual,,ALICE+ Wigner Scientific Computing Laboratory project leader,,30,Yes,No,Yes,Data Management; Software+ software development and software management; Training and careers,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2,10,yes,1 PB simulation + 1 PB real data,At multiple computing centres (for example+ in case of simulations); At multiple experimental sites,,10-100 GB/s,,10-100 GB/s,,SCP; XrootD,,Yes+ an in-house solution,,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...),,,,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...),,Files,ROOT,1 GB - 10 GB,,Sequential (read each file from start to end),,1 thousand - 1 million,,Multiple replicas on disk/tape; We rely on low level safety (RAID systems+ for example),,No need (data is public),,Yes+ we are fully committed to FAIR data,,Yes (no further questions asked),,Internally developed and operated catalogue(s),,Yes+ we emit PIDs internally in the experiment / initiative,,Follows the WLCG and CERN ALCIE requirements,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C#; C/C++; Fortran,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,SSE,Nvidia,FPGAs (Xilinx); FPGAs (Altera),,Cost/performance optimization; External decision (funders+ reviewers + ..),,We use a single / few platforms we program directly,,We use a GPL3 licensing model,Yes,We license our software as free and open source,,Fortan+ C+++ python+ cuda,ARM,More,More,More,Driver problems prevent us to use. Upgrades are more frequent+ only tested hardware+driver solutions can be include but his takes time. ,Yes,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,,Professional software engineers hired for this; Procured from external non-research entities,,Some do,Training provided via courses+ hackathons+ MOOC etc by the involved institutions; Training provided via courses+ hackathons+ MOOC etc by the experiment / initiative,Yes,Basic programming (languages+ development tools+ ...); Advanced programming (languages+ development tools+ ...); Principles of FAIR development; Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),,,No,,,,,,,,,,,,,,HPC (CPU and GPU); Cloud (CPU and GPU); Quantum Emulators (classical systems emulating quantum hardware); Storage systems (disks); Storage systems (tapes),,It is a WLCG Tier0/1/2/3; Part of a national infrastructure (please specify),Wigner Scientific Computing Laboratory,National level grants; Open only to some institutions' users,,10000,50,2,1,1MW,200,34,,Yes+ needed to access the compute node,,Yes+ upon verification and test from the site admins; Yes+ via containers,,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via Cloud interfaces (k8s+ Openstack+ ...),,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …),,2-6 months (6 excluded); 1-3 years (3 excluded),1-12 h (12 excluded); > 72h,,x84_64,,Nvidia; AMD,,16 - 31,>4,>8,> 50 GB,Yes (please explain),,Infiniband+ < 100 Gbps,Infiniband+ >100 Gbps,,,10-100 Gbps,LHCONE,,Private IPs,,Connection to other onsite compute nodes; Connection to a selected list of outside locations,Connection from other onsite compute nodes; Connection from a selected list of outside locations,No+ all left to the users,,,Xrootd; dCache; NFS; EOS,dCache,,POSIX (for example+ NSF mount or any other fuse mount); Xrootd,,Xrootd,,10-100 GB/s,10-100 GB/s,Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..); SW installation tools (Module+ Conda+ Guix+ APT+ RPM+ ...),,Yes,,Choice of cooling solution (adiabatic vs chillers+ …); Operational choices (node shutdown when not needed+ lower clock+ job packing to maximise systems which can be shut down+ ...); We continuously monitor the PUE of the site,,Yes+ we are doing it (please specify below); Yes+ we are tracing technologies to possibly do it in the near future
Hervé MOUTARDE,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),herve.moutarde@cea.fr,,Theoretical Nuclear Physics (NP),,Theoretical interpratation of experimental data. Model building. Statistical data analysis. Software development. ,Individual,,PARTONS,,10,Yes,Yes,No (will skip most of the technical sections),,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Assess what can actually be learnt about 3D proton structure from existing experimental data+ and derive meaningful uncertainties on estimated physical quantities. Forecast and design new experiments to improve knowledge of 3D proton structure.,https://inspirehep.net/literature/1840241,,Yes,Regularization of ill-posed inverse problems. Can quantitative information about 3D proton structure be extracted from experimental data? Does the solution exist? Is it unique? What could be the prior information needed for a meaningful interpretation of experimental data?,https://inspirehep.net/literature/1856752,,No,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Joaquin Gomez-Camacho,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),gomez@us.es,,Experimental Nuclear Physics (NP); Theoretical Nuclear Physics (NP),,Research in nuclear reactions and in experiments in small accelerator facilities.,Individual,,CNA,cna.us.es,,No (default),No,No (will skip most of the technical sections),,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mark Hodgkinson,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),m.hodgkinson@sheffield.ac.uk,,Experimental High Energy Physics (HEP),,ATLAS,Individual,,Developing software in ATLAS,,Several thousand,Yes,No,Yes,Computing Environment; Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...); aarch64,,Generic Linux 64bit; MacOs,,Local Disk needed; Shared disk areas needed,Yes+ on distributed file systems (for example+ CVMFS),,1 core; Full nodes (using all the cores),,0,,Via a shell+ possibly via a login node,,Via CVMFS,,Singularity / Apptainer; Other (please specify),,No,,,,,,No,No,No,Yes,No,No,Python; C/C++,,Gaudi+ Microsoft Visual Studio+ Gitlab,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; aarch64,SSE; AVX; AVX2; AVX512,Nvidia; AMD; Intel,FPGAs (Xilinx),,Cost/performance optimization,,We use a single / few platforms we program directly,,We use Apache 2.0 licensing model,,We license our software as free and open source,,CUDA/AlPaka.Sycl etc,Quantum Computing,At the same level,At the same level,More,reduce cost+ process more data,Yes,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software,,Some do,Training provided via courses+ hackathons+ MOOC etc by the involved institutions; Training provided via courses+ hackathons+ MOOC etc by the experiment / initiative,Yes,Basic programming (languages+ development tools+ ...); Advanced programming (languages+ development tools+ ...); Distribute computing (access to Grids+ storage systems+ AAI+ ...); Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),,,Yes+ but with limited career paths,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Frank Krauss,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),frank.krauss@durham.ac.uk,,Theoretical High Energy Physics (HEP),,Author of MC event generators,Individual,,SHERPA,,15,Yes,No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),Sherpa author .....,,x86_64,,,,,Previous experience in the developers’ base; External decision (funders+ reviewers + ..),,We use a single / few platforms we program directly,,We use a GPL3 licensing model,Yes,We license our software as free and open source,,,,More,More,More,Performance,No,Funding,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,SHERPA,,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ulrich Husemann,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),ulrich.husemann@kit.edu,,Experimental High Energy Physics (HEP),,Collider-based HEP: data analysis and detector development,Individual,,data analysis in CMS,,,Yes,No,Yes,Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,,Some do,Training provided via courses+ hackathons+ MOOC etc by the involved institutions; Training provided via courses+ hackathons+ MOOC etc by the experiment / initiative,Yes,Basic programming (languages+ development tools+ ...); Advanced programming (languages+ development tools+ ...); Distribute computing (access to Grids+ storage systems+ AAI+ ...); Principles of FAIR development; Efficient programming (profiling+ energy measurement+ ...); Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),,,Yes+ but with limited career paths,,Machine learning for data analysis: training and inference,,,No,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anon Y. Mous,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),anony@mous.com,,Experimental High Energy Physics (HEP),,Data analysis,Individual,,data analysis at ATLAS,,,No (default),No,Yes,Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...),,,x86_64; aarch64,,,,,Decision coming from resources deployed in the centre we need to use,,We use an internally designed system (for example+ macros),,,,We license our software as free and open source,,,,More,More,More,performance,No,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data volumes will increase,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,,No,Training provided via courses+ hackathons+ MOOC etc by the involved institutions; Training provided via courses+ hackathons+ MOOC etc by the experiment / initiative,No,Basic programming (languages+ development tools+ ...); Distribute computing (access to Grids+ storage systems+ AAI+ ...); Efficient programming (profiling+ energy measurement+ ...),,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Andre Sailer,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility); Research software engineer (writing+ testing and managing code for an initiative),andre.philippe.sailer@cern.ch,,Experimental High Energy Physics (HEP),,Project Management+ Software development for simulation and reconstruction+ distributed computing interfaces for computing and data management,Individual,,software and computing for Future Detector Studies (CLIC+ FCC+ ILC),key4hep.web.cern.ch https://ilcdirac-doc.web.cern.ch/,20 ish,Yes,No,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management,No,,Experiment/Institution SSO; Other (please provide details),Other: INDIGO IAMs hosted at CERN,username/password; X509; 2FA; tokens,,Yes (please comment how),CERN requires 2FA for login to SSO (in most cases),No,,Indigo-IAM,,I hope no more (voms-admin to IAM was bad enough)+ I guess there will be more use of tokens for data transfers,Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference,https://key4hep.github.io/key4hep-doc/  https://github.com/key4hep/ https://github.com/ilcsoft ,Interactive (shell/text); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),cern lxplus+ or ilcdirac,Single core,Directed Acyclic Graphs (DAGs),,A few hundred to few thousand CPU years per year,,,other (please specify),Mostly using opportunistic resources+ no large pledges,Grid facilities,,ilcdirac.cern.ch,Fair share on batch systems; Opportunistic,,DIRAC,,iLCDirac sends pilots+ pilots pull payloads+ dirac.readthedocs.org,,Other,BDII,Data downloaded by the job itself or its wrapper before execution,,Yes,,Geant4 simulation of events+ about 50%+ reconstruction of high energy events with backgrounds 50%,simulation for ILC+ reconstruction for CLIC,3h-1d,1-2 GB (2 excluded),20-100 GB,> 100 MB/s,no,10-100 MB/s,Not Applicable,I expect little changes to the model+ but increased need for computing time and data storage when certain studies ramp up activities,,10 PB,5,yes,10 PB are simulation,At multiple computing centres (for example+ in case of simulations),,Other (please specify),I don't know+ jobs shouldn't  take too long to upload their outfiles of a few hundred MB,,I don't know+ jobs shouldn't wait too long to download their input files of a few hundred MB,WebDAV; XrootD,,Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),DIRAC,Multiple file / object replicas; Tape long term storage,,Yes (FTS),,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...),,Files,ROOT (EDM4hep) SIO (LCIO) (https://github.com/ilcsoft/lcio),1 GB - 10 GB; 1 MB - 1 GB,,Sequential (read each file from start to end); Iterated (read each file multiple times); Read one file per job; Read multiple files per job,,1 million - 1 billion,,We accept a fraction of data loss,At least for MC files,Needs an AAI mechanism (see the Compute Environment later),,We have not investigated yet,,"No+ it needs to be complemented with external ""metadata"" information",,Use open source solutions (please specify),DIRAC Metadata catalog,Yes+ we rely on unique identifiers from the infrastructure (for example+ data identifiers from the data management systems),LFNs of the Dirac FileCatalog,DIRAC should be able to scale in our use case for the next 5 to 10 years+ as we are still far below LHCb scale,x86_64 (Intel+ AMD+ ...),,Specific linux versions (please specify); MacOs,alma9+ ubuntu,Local Disk needed,Yes+ on distributed file systems (for example+ CVMFS),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),1 core,,0,,I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),,Via CVMFS,,Singularity / Apptainer,,No,,No,,Yes+ if there in no outgoing connectivity from the compute nodes,,No,No,probably more aarch64,No,Use of GPUs,No,Python; C/C++,,,Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,,,,,Capability to execute of more centres,,We use a single / few platforms we program directly,,We use Apache 2.0 licensing model,No,We license our software as free and open source,,,aarch64,More,More,More,hardware constraints,No,person power,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Santiago González de la Hoz,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),santiago.gonzalez@ific.uv.es,,Experimental High Energy Physics (HEP),,computing in the ATLAS experiment at CERN,Individual,,IFIC-Valencia,,,No (default),No,Yes,Computing Environment,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit,,Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc),Yes,Full nodes (using all the cores),Nvidia GPUs,50,,Via a shell+ possibly via a login node,,Via CVMFS,,Singularity / Apptainer; Docker,,Yes+ the whole time,,Outgoing connectivity globally,,Yes+ we need services executing on special nodes,,Yes+ we need full root access,X11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Send jobs to the Grid resources+ reading data from different sites+ running the jobs on multicore WNs in a different sites and having the ouput in my local infrastructure,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Romain Reuillon,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),romain.reuillon@iscpif.fr,,Other non-physics related research domains (please specify below),Complex-systems+ social sciences,Modeling and Simulation,Individual,,OpenMOLE,openmole.org,30,No (default),No,Yes,Workload Management; Computing Environment; Software+ software development and software management,Yes,,,,,,,,,,,,,Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …); Machine Learning/AI training/inference; Other (please specify),Simulation Model Exploration,Via some services (for example via Web services as Jupyter notebooks); Other (please specify),OpenMOLE,Single core,Other,Computing workflows with cycle implemented in OpenMOLE translated as jobs submission in the platform,2000000 Hour/CPU,,,1 to 5 years,,HPC Centres; Institutional Clouds; Grid facilities,,Queuing managed by the policy of the target infrastructure,Fair share on batch systems; Opportunistic; Dedicated VMs or servers; Preemptable jobs,,DIRAC; Openstack,,OpenMOLE submit to DIRAC+ SLURM or any jobs scheduler on behalf of the user. The onlne version of OpenMOLE run the OpenMOLE application on top of OpenStack + k3s.,,Rely on GRID Provisioning; Rely on Cloud Provisioning,,Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution,,No,,,Calibration,<1 h,2-5 GB (5 excluded),,1-10 MB/s,,1-10 MB/s,,Steady+ but if multi-core job + high memory is available it can be useful for some models,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit,,Local Disk needed,No,No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),1 core; Multicore jobs on a single node (not using the while node),,0,,I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),,I will install myself; We use containers with preinstalled SW,,Singularity / Apptainer,,No,,Outgoing connectivity globally,,No,,No,No,,,,Everywhere Container support+ IPFS file access,Python; C/C++; Java; Fortran; R; Other (please specify); Julia,Scala and bash for OpenMOLE+ models are provided by the users in there own languages,,,,,x86_64,,,,,,,,,We use a GPL3 licensing model,,We license our software as free and open source,,,,At the same level,,,,Yes,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jyoti Prakash Biswal,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),jyoti.biswal@stfc.ac.uk,,Experimental High Energy Physics (HEP),,ATLAS Trigger GPUs+ Part of SWIFT-HEP and GridPP projects.,Individual,,Making GPUs more usable at ATLAS,,4,Yes,Yes,Yes,Software+ software development and software management,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,Yes+,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); HEP specific software (ROOT+ dd4hep+...),,,Other,AVX512,Nvidia; AMD,Other,,Cost/performance optimization; Need for more performance than simple CPUs could provide,,We use a single / few platforms we program directly,,No,,We keep it for internal use only,,,,At the same level,More,More,,Yes,,Yes+ because data will become more complex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bruno Giacomazzo,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),bruno.giacomazzo@unimib.it,,Theoretical Gravitational Waves (GW),,Numerical Simulations of Gravitational Wave Sources,Individual,,TEONGRAV,https://web.infn.it/CSN4/index.php/it/17-esperimenti/195-teongrav-home,100,Yes,No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Fortran,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,,Nvidia,,,Decision coming from resources deployed in the centre we need to use,,We use frameworks / toolkits (please specify),Einstein Toolkit,We use a GPL3 licensing model,No,We license our software as free and open source,,,,More,No idea / Not applicable,More,,No,personnel,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leif Lönnblad,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Other (please specify),leif.lonnblad@fysik.lu.se,Author of scientific software,Theoretical High Energy Physics (HEP); Theoretical Nuclear Physics (NP),,Author of general purpose event generators and related tools,Individual,,Pythia,pythia.org,18,Yes,Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,,Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; ppc64le,,Nvidia,,,,,,,We use a GPL3 licensing model,Yes,We license our software as free and open source,,C++ core with optional python user interface,Currently experimenting with GPU,More,No idea / Not applicable,More,Performance,Yes,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex,,,,,,,,,,,,,Simulating collective effects in heavy ion collisions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Luis Sarmiento,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Research software engineer (writing+ testing and managing code for an initiative),Luis.Sarmiento_Pico@fysik.lu.se,,Experimental Nuclear Physics (NP),,Top to bottom experimental nuclear physics. From DAQ to published results.,Individual,,data analysis GSI/FAIR-0,,4,No (default),No,Yes,Data Management; Computing Environment; Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.5,5,burst in experimental campaigns,0.4 data 0.1 simulations,At a single experimental site; At a single computing centre (for example+ in case of simulations),,1-10 GB/s,,1-10 GB/s,,SCP; POSIX (via mounted storage areas),,No+ using directly low level protocols,,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...),,No,,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...),,Files,LMD+ ROOT,1 GB - 10 GB,,Sequential (read each file from start to end),,1 thousand - 1 million,,Multiple replicas on disk,,No need (data is public); Data embargos,,We are transitioning to data managed via FAIR principles,,"No+ it needs to be complemented with external ""metadata"" information",,Internally developed and operated catalogue(s),,No,,Funding agencies will required data to be public but no funding will be allocated no nothing will happen,x86_64 (Intel+ AMD+ ...); aarch64,,Windows; Generic Linux 64bit,,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc); Yes+ on the local compute node (for example+ $HOME etc); Yes+ on distributed file systems (for example+ CVMFS),Yes,Multicore jobs on a single node (not using the while node),,,,Via a shell+ possibly via a login node,,Via Module; We use containers with preinstalled SW,,Singularity / Apptainer,,No,,No,,No,,No,VNC,NO,NO,more pressure for public data,NO,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; aarch64,,,,,Cost/performance optimization,,We use a single / few platforms we program directly; We use an internally designed system (for example+ macros),,No,Yes,We distribute software only under dedicated agreements,,EPICS,,More,No idea / Not applicable,More,required for performance,No,"People power. Difficult to get money for ""just developing/improving"" as opposed to a researcher doing their best",Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software,,Some do,No specific training,No,Basic programming (languages+ development tools+ ...); Advanced programming (languages+ development tools+ ...),,Heterogeneous computing+ ,No,,Digest listmode data looking for a handful of correlated events with different timestamps ,,,No,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vitaly Magerya,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),vitalii.maheria@kit.edu,,Theoretical High Energy Physics (HEP),,Writing software for theoretical high-energy physics (multi-loop calculations)+ running it+ managing the local computing cluster.,Individual,,KIT,,,No (default),No,Yes,Workload Management; Computing Environment; Software+ software development and software management,Yes,,,,,,,,,,,,,Model/theory calculations; Monte Carlo simulations; Machine Learning/AI training/inference,,Interactive (shell/text); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),,Single core; Multi/Many cores (on a single node),Directed Acyclic Graphs (DAGs),,Unpredictable in advance,3000000 on A100,High RAM size on CPU machines (1TB+),1 to 6 months,A project might last for several months to a year+ but to correctly gauge it+ we need the computing resources fast: in a week or two. Preferably immediately (otherwise we need to use our own infrastructure).,HPC Centres; Dedicated clusters (for example owned by the initiative),,SLURM (slurm.schedmd.com),Fair share on batch systems,,,,,,Static List; Online resource catalogue(s) maintained by collaborators,,From shared filesystems at the computing centre; Data downloaded programmatically before execution+ to local/centre disks,,Yes,Not on our local cluster+ but for external clusters it would be very useful to coordinate with the local one.,We have several totally different workflows for different computations: short (minutes) CPU intensive jobs (1-2GB/CPU+ ~4CPUs/job) followed by long (hours to days) GPU jobs; long (hours to months) CPU and RAM intensive jobs (300-2000GB/job+ 32+CPUs/job+ 1TB+ disk space).,Numerical evaluation of Feynman integrals; solution of IBP systems,3h-1d,2-5 GB (5 excluded),> 100 GB,1-10 MB/s,No,None,Not Applicable,The requirements always grow.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,Generic Linux 64bit,,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc); Yes+ on the local compute node (for example+ $HOME etc),No+ no need,1 core; Multicore jobs on a single node (not using the while node); Full nodes (using all the cores),Nvidia GPUs,100,We have jobs that are 100% GPU+ and jobs that are 100% CPU.,Via a shell+ possibly via a login node,SSH with + and please+ for the love of GOD+ no 2FA -- those things destroy any chance at automation.,Preinstalled by the sysadmins; I will install myself; Via Module,,Singularity / Apptainer; Not needed+ but we can use them,,Other (please specify),If the system is up to data and most usual packages are installed+ then we don't need it.,No,To setup all the software and data we need free outgoing network access on the login node+ and the permission to use the login node to compile the software+ do light data processing. Incoming network access to the login node is a big bonus. Worker nodes only need to communicate with the networks discs.,No,,No,No,No,No,Maybe,Maybe,Python; C/C++; Fortran; Julia,CUDA,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang); Other (please specify),Wolfram Mathematica,,x86_64,SSE; AVX; AVX2,Nvidia,,,Cost/performance optimization; Need for more performance than simple CPUs could provide,,We use a single / few platforms we program directly,,No,No,We license our software as free and open source,,,We adapt to the available hardware+ if there is performance/cost benefit.,More,More,More,Performance requirements of the project at hand,Yes,Manpower,Yes+ because data will become more complex; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,(Quasi) Monte Carlo integration of Feynman integrals,https://github.com/gudrunhe/secdec,,Yes,Solving big systems of IBP relations between Feynman integrals,https://kira.hepforge.org,,No,,,,Yes,HTC CPU only (Grid-style+ batch+ ...); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks),We run a local cluster for the institute,No,,Other (please specify),To get access one must join or collaborate with the institute,5000,22,1,0,-,~100,6,,Yes+ needed to access the compute node,,No; Other,The admins install software on request; users are free to build their own,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...),,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Interactive “shell only”; Interactive “graphical” (for example via X+ VNC+ ...),,Other,> 72h,No hard limits on resources+ dynamic priorities for jobs,x84_64,,Nvidia,A100 GPUS,32 - 64,>4,2-4,> 50 GB,No,We have different machine classes for different needs; separate partitions with 1GB/CPU machines+ 5GB/CPU+ 20GB/CPU.,Standard ethernet+ 10 Gbps,Standard ethernet+ 10 Gbps,,,1-10 Gbps,No,,Private IPs,,Connection to onsite storage nodes; Connection to other onsite compute nodes; Connection to special onsite services; Connection to a selected list of outside locations; Other,Connection from other onsite compute nodes; Connection from special onsite services,No+ all left to the users,,,,,,POSIX (for example+ NSF mount or any other fuse mount),,,,< 1 GB/s,1-10 GB /s,Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..),,Yes,We are constrained by energy consumption and heat dissipation.,Choice of low power/performance machines at procurement; Operational choices (node shutdown when not needed+ lower clock+ job packing to maximise systems which can be shut down+ ...); We continuously monitor the PUE of the site,,Yes+ we are doing it (please specify below)
Stefan Dittmaier,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),stefan.dittmaier@physik.uni-freiburg.de,,Theoretical High Energy Physics (HEP),,Electroweak precision physics+ Higgs physics+ phenomenology of Standard Model extensions+ higher-order calculations+ perturbative quantum field theory,Individual,,precision calculations for collider physics,,12,Yes,No,Yes,Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Fortran; Other (please specify),,Mathematica,Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); Quantum computing tools and libraries (kiskit+ cirq+ ...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang); Other (please specify),Mathematica+ Form,,x86_64,,,,,Cost/performance optimization,,We use a single / few platforms we program directly,,No,No,We license our software as free and open source,,C+++ Fortran+ Mathematica+ Form,,At the same level,No idea / Not applicable,No idea / Not applicable,,Yes,,Yes+ because of a shift in scientific needs,Researchers dedicating a fraction of their time to software,,Other (please specify),IT group and computing centre of the university,Some do,No specific training,No,Basic programming (languages+ development tools+ ...),,more regular programming courses in the regular BSc and MSc study programmes at universities,Yes+ but with limited career paths,,,,,No,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reinhard Alkofer,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),reinhard.alkofer@uni-graz.at,,Theoretical High Energy Physics (HEP); Theoretical Nuclear Physics (NP),,Physics of light and heavy mesons+ confinement and chiral symmetry breaking+ Schwinger effect in ultra-high field lasers+ ultraviolet completion of Standard model+ coupling of fermions to gravity,Individual,,Strong interactions in continuum quantum field theory,,5,No (default),No,Yes,Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Fortran; Julia,,,Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,,,,,,,,We use a single / few platforms we program directly,,No,Yes,We put our software in the public domain,,,,More,More,At the same level,performance,Yes,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,Researchers dedicating a fraction of their time to software,,Researchers dedicating a fraction of their time to software,,Yes,Training provided via courses+ hackathons+ MOOC etc by the involved institutions,Yes,Advanced programming (languages+ development tools+ ...); Principles of FAIR development; Efficient programming (profiling+ energy measurement+ ...); Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),,,Yes+ with good career paths,,Solution of systems of integro-differential equations,,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Johan Bijnens,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),johan.bijnens@fysik.lu.se,,Theoretical High Energy Physics (HEP); Theoretical Nuclear Physics (NP),,Flaviyur physics+ hadronic physics+ chiral perturbation theory,Individual,,Scientific computing both numerical and analytical,,,Yes,No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Other (please specify),Mathematica and the FORM analytical calculation tools,No,Python ecosystem (NumPy+ scikit-learn+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,,,,,Cost/performance optimization,,We use a single / few platforms we program directly,,We use a GPL3 licensing model,Partially,We license our software as free and open source,,,,At the same level,No idea / Not applicable,No idea / Not applicable,,Yes,,No,,,,,,,,,,,,,Numerics for chiral perturbation theory,https://github.com/johanbijnens/CHIRON,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
John Frankland,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Research software engineer (writing+ testing and managing code for an initiative),john.frankland@ganil.fr,,Experimental Nuclear Physics (NP),,Developing & maintaining software for data analysis of INDRA-FAZIA data for the collaboration's & my own research needs,Individual,,KaliVeda heavy-ion collisions data analysis toolkit,https://kaliveda.in2p3.fr/,3-4,No (default),No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,C/C++; Fortran,,,HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,,,,,Decision coming from resources deployed in the centre we need to use,,,,No,Yes,We put our software in the public domain,Using IN2P3-managed gitlab: https://gitlab.in2p3.fr/kaliveda-dev,,,More,No idea / Not applicable,No idea / Not applicable,required for performance,Yes,,Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Harald W Griesshammer ,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),hgrie@gwu.edu,,Theoretical Nuclear Physics (NP),,Theoretical low energy few nucleon physics ,Individual,,George washington University ,,3,Yes,No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; Fortran; R,,No+ we avoid at all costs ,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); Other (please specify),Naglib+ numerical recipes,,x86_64,SSE; AVX512; NEON; MMA,Nvidia; AMD; Intel,,,Cost/performance optimization; Previous experience in the developers’ base,,We use a single / few platforms we program directly; We use an internally designed system (for example+ macros),,We use a GPL3 licensing model,No,We keep it for internal use only,,,,More,No idea / Not applicable,More,,No,,Yes+ because data will become more complex; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,,,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BOUVET,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),dbouvet@in2p3.fr,,Experimental High Energy Physics (HEP),,Technical coordinator of LCG-France which is CNRS/IN2P3 project for organising and coordinating the French participation in WLCG project for the LHC.,Individual,,LCG-France,,,Yes,Yes,Yes,Data Management; Computing Environment,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Exponential growth,,At a single experimental site; At multiple computing centres (for example+ in case of simulations); At multiple experimental sites,,100-1000 GB/s,,100-1000 GB/s,,iRODS; SCP; POSIX (via mounted storage areas); GRIDFTP; S3; WebDAV; XrootD,,Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...); Tape long term storage; Caches to optimize access time,,Yes (FTS),,Cloud storage (Amazon S3+ Openstack Swift+ Oracle Cloud Storage+ Microsoft Azure storage+ ... ); Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...); Relational Databases (e.g. SQL); Non relational Databases (e.g. noSQL),,Files; DB records,,,,Sequential (read each file from start to end); Random (read specific parts of each file); Iterated (read each file multiple times); Read one file per job; Read multiple files per job,,> 1 billion,,Multiple replicas on disk; Multiple replicas on disk/tape; We rely on low level safety (RAID systems+ for example),,No need (data is public); Needs an AAI mechanism (see the Compute Environment later); Data embargos,,We are transitioning to data managed via FAIR principles,,"No+ it needs to be complemented with external ""metadata"" information",,Use open source solutions (please specify),,Yes+ we emit PIDs internally in the experiment / initiative,,,x86_64 (Intel+ AMD+ ...); aarch64,,Specific linux versions (please specify),Currently RHEL9 OS like,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc); Yes+ on the local compute node (for example+ $HOME etc); Yes+ on distributed file systems (for example+ CVMFS),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),Multicore jobs on a single node (not using the while node),Nvidia GPUs; AMD GPUs; Intel GPUs,20,,Via a shell+ possibly via a login node; I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),,Preinstalled by the sysadmins; I will install myself; Via Module; We use containers with preinstalled SW; Via CVMFS,,Singularity / Apptainer,,No,,Outgoing connectivity only to nodes in the local centre; Outgoing connectivity towards a selected number of subnets; Outgoing connectivity globally; Ingoing connectivity only from the local centre,,Yes+ we need services executing on special nodes,,Yes+ we need to be able to execute certain commands (as sudoers+ for example),VNC; X11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,No,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paolo Napolitani,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),paolo.napolitani@ijclab.in2p3.fr,,Theoretical Nuclear Physics (NP),,Nuclear reaction models and coding,Individual,,nuclear theories for intermediate energies (GANIL+ LNS),,,,,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,C/C++; Fortran,,No+ I intentionally avoid licensed tools,Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,,,,,,,,,,,Partially,We distribute software only under dedicated agreements,,,,More,No idea / Not applicable,No idea / Not applicable,required for performance (calculation speed speed),No,,Yes+ because data will become more complex; Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ulrich Mosel,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),mosel@physik.uni-giessen.de,,Theoretical Nuclear Physics (NP),,Working on models simulating nuclear reactions and on neutrino generators,Individual,,Neutrino generators,,,Yes,Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; Fortran,,,Python ecosystem (NumPy+ scikit-learn+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..),,,x86_64,,,,,Cost/performance optimization; Need for more performance than simple CPUs could provide,,We use a single / few platforms we program directly,,No,No,We put our software in the public domain,,Fortran+ Python,,At the same level,No idea / Not applicable,No idea / Not applicable,,Yes,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data volumes will increase,,,,,,,,,,,,,neutrino generator GiBUU,gibuu.hepforge.org,,,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Francesco Sanfilippo,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),francesco.sanfilippo@infn.it,,Theoretical High Energy Physics (HEP),,Lattice Quantum Chromodynamics,Individual,,INFN,,10,Yes,Yes,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,C/C++; Other (please specify),Bash and GNU suites,,Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …),,,x86_64,SSE; AVX; AVX2; AVX512,Nvidia,,,Cost/performance optimization; Need for more performance than simple CPUs could provide; Other,Availability of dedicated libraries on CUDA,We use an internally designed system (for example+ macros),,We use a GPL3 licensing model,Yes,We put our software in the public domain,,Metaprogrammed C++,,More,More,More,Enlarging the reach of our simulations,No,Developers,Yes+ because of a shift in scientific needs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Riccardo Torre,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory),riccardo.torre@ge.infn.it,,Theoretical High Energy Physics (HEP),,Phenomenology of Physics Beyond the SM and applications of ML to HEP,Individual,,Precision Machine Learning for HEP (PML4HEP),https://web.infn.it/CSN4/index.php/it/17-esperimenti/246-pml4hep-home,30,Yes,No,Yes,Workload Management; Data Management; Computing Environment; Software+ software development and software management,No,,,,,,,,,,,,,Model/theory calculations; Monte Carlo simulations; High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference,,Interactive (shell/text); Via some services (for example via Web services as Jupyter notebooks); Other (please specify),VSCode SSH remote + jupyter,Multi node; Multi/Many cores (on a single node); Whole node,,,200000,100000,,1 year,,HPC Centres; Other (please specify),Leonardo (Cineca),Mostly VSCode + RemoteSSH,Fair share on batch systems,,Other (please specify),SLURM,,,Nodes / centres registering directly the computing effort (for example+ HTCondor startds joining a schedd),SLURM commands such as squeue+ sinfo+ sacct+ etc.,From shared filesystems at the computing centre; Remote access via streaming; Other (please specify),Transfer data with scp and other services,No,,I login into leonardo+ allocate nodes with SLURM salloc+ then connect VSCode remote SSH directly to the computing nodes and execute jupyter notebooks interactively,,3h-1d,1-2 GB (2 excluded),1- 20 GB,> 100 MB/s,Python environments and tensorflow code/packages usually require low latency (importing libraries processes a large amount of small files),None,2-16 nodes,Development of more efficient parallelized GPU computing (OpenMPI+ etc),,0.01,1,I do not work for experiments+ so do not manage large amount of data (with large I mean > 10Tb),,At a single computing centre (for example+ in case of simulations),,> 1 TB/s,,> 1 TB/s,,SCP,,No+ using directly low level protocols,,Tiered storage systems (NVMe+ SSD+ HDD+ ...),,Yes (please specify),Leonardo data mover infrastructure with scp,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...),,Files,,1 GB - 10 GB,,Sequential (read each file from start to end); Random (read specific parts of each file),,1 thousand - 1 million,,We rely on low level safety (RAID systems+ for example),,No need (data is public),,Yes+ we are fully committed to FAIR data,,"No+ it needs to be complemented with external ""metadata"" information",,Internally developed and operated catalogue(s),,No,,,x86_64 (Intel+ AMD+ ...); Other (please specify),NVidia GPUs,Generic Linux 64bit,,Local Disk needed; Shared disk areas needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),Multicore jobs on a single node (not using the while node); Full nodes (using all the cores),Nvidia GPUs; Quantum Computing Emulators,80,,Via a shell+ possibly via a login node,,I will install myself; Other (please specify),I manage python environments locally for my user,Docker,,No,,Outgoing connectivity globally; Ingoing connectivity globally (a public node),,No,,No,X11; Other,,,,,Python,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang); Other (please specify),Mathematica,,x86_64,,Nvidia,,,Capability to execute of more centres; Previous experience in the developers’ base,,,,No,Yes,We put our software in the public domain,,Python,,At the same level,No idea / Not applicable,More,,Yes,,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data volumes will increase; Yes+ because of a shift in scientific needs,,,,,,,,,,,,,,,,,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Jay Vijay Kalinani,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),jaykalinani@gmail.com,,Theoretical Gravitational Waves (GW); Other physics related domains (please specify below),Numerical relativity,Simulating mergers of binary system of compact objects,Individual,,TEONGRAV ,,,,,Yes,Computing Environment,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,x86_64 (Intel+ AMD+ ...),,,,Local Disk needed,Yes+ on shared disks (for example+ $HOME+ $WORK+ etc); Yes+ on the local compute node (for example+ $HOME etc); Yes+ on distributed file systems (for example+ CVMFS),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),Full nodes (using all the cores); Many nodes needed for the same workflow+ with node-to-node communication,Nvidia GPUs; AMD GPUs; Intel GPUs,98,,Via a shell+ possibly via a login node,,Preinstalled by the sysadmins; Via Spack,,Singularity / Apptainer; Docker,,No,,No,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Running simulations of compact binary mergers,,,No,,,,,,,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Giorgio Dho,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Research software engineer (writing+ testing and managing code for an initiative),giorgio.dho@lnf.infn.it,,Experimental High Energy Physics (HEP); Observational Astroparticle (not RA or GW),,I use python and C++ code to analyse data taken by low rate experiments,Individual,,data analysis and simulation at CYGNO,,30,Yes,No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,VS Code,Python ecosystem (NumPy+ scikit-learn+ …); Simulation toolkits (geant+ fluka+ corsika+ …); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),OneAPI TBB,,x86_64,,Nvidia,,,Cost/performance optimization,,We use an internally designed system (for example+ macros),,No,Partially,We put our software in the public domain,,,,More,At the same level,More,Required performance,Yes,,Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gernot Maier,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),gernot.maier@desy.de,,Observational Astroparticle (not RA or GW),VERITAS Gamma-ray observatory,Responsible for one analysis chain; responsible for Monte Carlo and IRF Production Chains,Individual,,Monte Carlo and data analysis for the VERITAS gamma-ray observatory,,5,Yes,No,Yes,Data Management; Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0 PB,15,Yes.,Roughly 50% are observed data+ 10% calibration data+ 40% simulations,At multiple computing centres (for example+ in case of simulations),,10-100 MB/s,,,,,,Yes+ a commercial solution (e.g. GlobusOnline),,,,No,,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...); Relational Databases (e.g. SQL),,Files; DB records,Custom experiment specific format for observational data (VBF); otherwise ROOT+ FITS,10 GB - 100 GB,,Sequential (read each file from start to end),,1 million - 1 billion,,Multiple replicas on disk,,Needs an AAI mechanism (see the Compute Environment later),,We are transitioning to data managed via FAIR principles,,"No+ it needs to be complemented with external ""metadata"" information",,Internally developed and operated catalogue(s),,No,,In the process of developing a data management plan for VERITAS+ as observatory will shut down in the next 3-6 years; requires sunset procedures to ensure long-term availability and usability of data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); HEP specific software (ROOT+ dd4hep+...),,,x86_64,SSE; AVX2; AVX512,,,,Cost/performance optimization,,We use a single / few platforms we program directly,,No,Partially,We keep it for internal use only,(We are in the process of making as much software as possible public),Observatory is in “sunset” mode - no major software developments planned.,No.,Less,At the same level,No idea / Not applicable,,No,Manpower,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,,,,,,,,,,,,,Calibration+ reconstruction+ and analysis of gamma-ray data from VERITAS observatory,https://zenodo.org/records/10980673,,Yes,Simulation pipeline for the VERITAS Observatory,https://zenodo.org/records/10809968,,No,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Alessandro,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),camilletti92@gmail.com,,Other physics related domains (please specify below),High energy astrophysics,Numerical simulations of binary neutron star mergers,Individual,,BNS mergers,,,No (default),No,Yes,Software+ software development and software management,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++,,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64,AVX512,,,,Cost/performance optimization,,We use a single / few platforms we program directly,,,,We license our software as free and open source,,,,At the same level,At the same level,No idea / Not applicable,,No,core hours,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),ee@sss.com,,Theoretical High Energy Physics (HEP),,d,Individual,,fd,,,,,Yes,Software+ software development and software management,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python,,gf,HEP specific software (ROOT+ dd4hep+...),,,aarch64,AVX2,,,,Capability to execute of more centres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Quantum Emulators (classical systems emulating quantum hardware),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ewrwe,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),fdsfd@dsds.vv,,Theoretical High Energy Physics (HEP),,ewe,Individual,,ew,ew,ew,,,Yes,Authentication and Authorization; Workload Management; Software+ software development and software management; Training and careers,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dsadas,Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),dasad@ddd.cvf,,Theoretical High Energy Physics (HEP),,ewq,Individual,,ewq,,,,,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management; Training and careers,No,,,,,,,,,,EGI-Checkin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,> 129 nodes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes+ we emit PIDs internally in the experiment / initiative,,,,,,,,,,,,,,,,,,,,,,,,,,,X11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Yes+ because data volumes will increase,,,,,,,,,,,Yes+ but with limited career paths,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Giovanni Mazzitelli,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),giovanni.mazzitelli@lnf.infn.it,,Experimental High Energy Physics (HEP); Observational Astroparticle (not RA or GW),,directional dark matter search,Individual,,CYGNO,https://web.infn.it/cygnus/,55,Yes,No,Yes,Workload Management,No,,,,,,,,,,,,,Monte Carlo simulations; Near real time processing; Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference,https://github.com/CYGNUS-RD/middleware,Via some services (for example via Web services as Jupyter notebooks); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),,Multi/Many cores (on a single node),No,,O(1000) cores,,,1 to 5 years,,Institutional Clouds; Grid facilities,,,Fair share on batch systems; Dedicated VMs or servers,,Openstack; HTCondor,,,,Rely on Cloud Provisioning,,From shared filesystems at the computing centre; Data downloaded by the job itself or its wrapper before execution; Remote access via streaming,,Yes,,raw data images clustering and filtering+ digitalization in simulation ,CYGNO,<1 h,1-2 GB (2 excluded),20-100 GB,1-10 MB/s,,1-10 MB/s,2-16 nodes,raw data throughput grow up of six time in next two year,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,https://github.com/CYGNUS-RD/middleware,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Graeme Stewart,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),graeme.andrew.stewart@cern.ch,,Experimental High Energy Physics (HEP),,Software developer at CERN,Individual,,Generic experiment software R&D,,6,Yes,Yes,Yes,Software+ software development and software management; Training and careers,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Python; C/C++; Julia,Julia is very much an R&D activity - no production use yet,,Python ecosystem (NumPy+ scikit-learn+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; aarch64,SSE; NEON; AVX; AVX2; AVX512,Nvidia,,,Cost/performance optimization,,We use frameworks / toolkits (please specify),Julia supports generic GPU programming via KernelAbstractions,We use Apache 2.0 licensing model,Partially,We license our software as free and open source,,Julia R&D is promising and may grow,RISC-V,At the same level,At the same level,More,Better value for money for out computing problems,No,People,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this; Procured from external non-research entities,As much open souce tooling used as possible,Some do,Training provided via courses+ hackathons+ MOOC etc by the involved institutions; Training provided via courses+ hackathons+ MOOC etc by the experiment / initiative,No,Basic programming (languages+ development tools+ ...); Advanced programming (languages+ development tools+ ...); Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),,,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zach Marshall,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of a scientific initiative (for example+ an experiment+ an instrument+ an observatory); Research software engineer (writing+ testing and managing code for an initiative),ZLMarshall@lbl.gov,,Experimental High Energy Physics (HEP),,I am the computing coordinator of the ATLAS Experiment at CERN+ one of the four major LHC experiments,Representing an initiative or a centre,The ATLAS Experiment at CERN,ATLAS,atlas.cern,6000,Yes,No,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management; Training and careers,No,,Experiment/Institution SSO; Federated (for example+ via eduGain); Username / Password,IAM and Globus grid certificates for central services+ as well as CERN SSO,username/password; X509; 2FA; tokens,In the process of migrating away from certificates,Yes (please comment how),Many data centers require 2FA+ including CERN+ at which many of our services sit.,Yes,Most services are not required to provide a second factor during authentication; only users are.,EGI-Checkin; Commercial Providers (e.g. Google+ Facebook+ Apple+ Github+ ...); Indigo-IAM; CERN-SSO,Commercial provider authentication is mostly for edge use cases (e.g. discussion forums+ open data access+ etc),Move further towards tokens and away from certificates. Increase in federated identity based access to facilities. ,Model/theory calculations; Monte Carlo simulations; Near real time processing; Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference,https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,Interactive (shell/text); Via some services (for example via Web services as Jupyter notebooks); Interactive (graphical - for example using X or VNC); Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),All of the above. Significant use via ssh. Jupyter is clearly growing+ especially for instruction.,Single core; Multi/Many cores (on a single node); Whole node,Directed Acyclic Graphs (DAGs),We are able to support all of these (and do); the most common submission is many simple 8-core jobs managed by our job submission infrastructure.,7500 MHS23 or about ~600k cores continuously+ +10-15% if we include local analysis needs.,Still hard to be precise; I would estimate around 1M currently+ but growing.,,1 to 5 years,We have annual requests+ but also provide long-term projections (e.g. https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/UPGRADE/CERN-LHCC-2022-005/),HPC Centres; Commercial Clouds; Institutional Clouds; Grid facilities; Dedicated clusters (for example owned by the initiative),We use all the CPU we can+ including all of these facilities,https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,Fair share on batch systems; Opportunistic; Preemptable jobs,"Grid and local systems mostly use something like fair share. HPCs are used more opportunistically+ but often with a substantial allocation (so saying ""opportunistic"" isn't really correct). On commercial clouds and some other systems we use preemptable jobs to keep costs down.",Openstack; HTCondor; Panda; k8s,https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02  (Note that OpenStack+ k8s+ and HTCondor are all used at different points in the production system+ even if PanDA is our most 'famous' product).,"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02
https://doi.org/10.1007/s41781-024-00114-3",,Rely on GRID Provisioning; Rely on Cloud Provisioning; Online resource catalogue(s) maintained by collaborators; Nodes / centres registering directly the computing effort (for example+ HTCondor startds joining a schedd),It really depends on the resource type,From local disks; From shared filesystems at the computing centre; Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution; Remote access via streaming,Depends on the workload type and center. We have solutions that work with different levels of data and service availability. Normally we provide software via cvmfs (cached filesystem)+ data via shared file systems+ output to local disks (and then copied to a remote location). Other options are all part of the ecosystem.,Yes,Ultimately they do require such access+ but we have ways to work around limited access from a specific compute node (e.g. through edge services),Monte Carlo simulation is about half our resources. It is relatively low I/O (particularly low input)+ reasonably low memory (usually <1 GB/core for 8-core jobs)+ CPU-intensive+ and relatively low communication. Embarrassingly parallel. It is constructed to consume about 12h per job+ but we can tune the number of events in a job to adjust the timing. The transfers do not require significant networking+ but they usually come at the beginning or end of a job+ and we want them to execute quickly. So the average over many jobs running asynchronously is not much higher than the requirement for a single job.,Full simulation at ATLAS,3h-1d,< 1 GB,1- 20 GB,> 100 MB/s,These jobs do not need to be low latency. Generally a few days to a week is fine.,> 100 MB/s,2-16 nodes,I expect the number of cores per job to increase. General changes to the model are described in https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/UPGRADE/CERN-LHCC-2022-005/,,1 EB,15 so far,No; when we reach the HL-LHC there will be a rapid increase,https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,At a single experimental site; At multiple computing centres (for example+ in case of simulations),,10-100 GB/s,6-8 GB/s for real detector data from the experiment; MC simulation is created worldwide at all times and is significantly larger.,> 1 TB/s,We have 100 sites+ so this is less than it sounds like. Again+ much information in https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,POSIX (via mounted storage areas); S3; XrootD,https is also used for some cases.,Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),Rucio predominantly; see https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...); Tape long term storage; Caches to optimize access time,Tiered storage is not built into the data model+ but is used when we have e.g. specific workflows that require high I/O or iops. ,Yes (FTS),,Cloud storage (Amazon S3+ Openstack Swift+ Oracle Cloud Storage+ Microsoft Azure storage+ ... ); Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...); Relational Databases (e.g. SQL); Other (please specify),We also have significant Oracle database infrastructure+ e.g. for conditions+ as well as Hadoop and a few other technologies.,Files,Custom RAW format+ most files in ROOT format (by volume)+ many HDF5 (for ML applications especially)+ and lots of little other file types (TXT for logs and LHE+ csv+ etc),1 GB - 10 GB,,Sequential (read each file from start to end); Random (read specific parts of each file); Read one file per job; Read multiple files per job,Depends on the job type,> 1 billion,,Multiple replicas on disk; Multiple replicas on disk/tape; We rely on low level safety (RAID systems+ for example),Data centers use RAID; globally we use disk/tape replicas depending on the data type,Needs an AAI mechanism (see the Compute Environment later); Data embargos,There would not be a crisis if much of our data were accidentally made public+ but we protect the experiment data in general. We also offer open data+ separately.,Yes+ we are fully committed to FAIR data,,Yes (no further questions asked),As much as possible our data are self-describing.,Internally developed and operated catalogue(s),AMI+ in-file metadata catalogs+ and various other metadata stores; see https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,Yes+ we rely on unique identifiers from the infrastructure (for example+ data identifiers from the data management systems),Rucio provides a GID for each file which is used,There are lots of predictions; the only clear thing is that we'll have more data. See https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/UPGRADE/CERN-LHCC-2022-005/,x86_64 (Intel+ AMD+ ...); aarch64,,Specific linux versions (please specify),centos7+ alma9 (and equivalent distributions)+ MacOS via Lima or other containers.,Local Disk needed; Shared disk areas needed,No,No+ no need,1 core; Multicore jobs on a single node (not using the while node); Full nodes (using all the cores),,5,There is only limited GPU offload today+ though many developments ongoing. ML applications offload significant work to GPUs.,Via a shell+ possibly via a login node; I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),Different users have different access patterns. Production managers do not log in. Users do.,We use containers with preinstalled SW; Via CVMFS,We have the option to use containers+ but prefer cvmfs for its flexibility and reliability,Singularity / Apptainer; Docker; Shifter,We use containers for our workloads and can work with many different solutions.,No,,No,We can work around a lack of network access; if it's provided+ we can make use of it.,Yes+ we need services executing on special nodes; Yes+ if there in no outgoing connectivity from the compute nodes; Yes+ if there in no incoming connectivity to the compute nodes,We run a variety of services including data access and workload management services that need to talk to the outside world. We can deploy those as edge services if needed.,We need the sysadmins to start services for us+ no direct control,No,I expect growing use cases for GPUs,I don't see significant changes+ but more flexibility as we adapt to diverse resources,There is potential for a growing hunger for GPUs in the coming years.,I don't see significant changes+ but more flexibility as we adapt to diverse resources,Python; C/C++,Primarily python and C++; some code in Java and Fortran,Many users have IDEs that they like; as an experiment we do not rely on them. We are in discussions with Intel and NVIDIA to provide distributed versions of their compilers+ runtime libraries+ IDEs+ etc in our experiment so that they can be used outside of CERN (which restricts our developers currently),Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),See https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,,x86_64; aarch64,SSE; AVX; AVX2; AVX512,Nvidia; AMD; Intel,TPUs; FPGAs (Xilinx); FPGAs (Altera),There's a lot of things being tested+ but our production system is fairly vanilla until we see some significant gain,Cost/performance optimization; Capability to execute of more centres; Decision coming from resources deployed in the centre we need to use,We have more control over our own Grid sites+ but also deploy on HPC systems where we don't get to select the hardware. Most of the optimization comes down to cost/benefit analyses (e.g. if we deploy new instruction sets how many CPUs do we exclude),We use frameworks / toolkits (please specify),We support a number of portability languages and layers+ and compile for multiple platforms; see https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2022-02,We use Apache 2.0 licensing model,Yes,We put our software in the public domain,Our software is open source+ including software used for the build system and externals,"C++ and Python will continue to drive our code. We will integrate some portability languages for accelerator use+ and currently support several. I expect once these features are sufficiently well integrated into C++ and gcc+ we may go back to ""just"" C++ and Python.",Yes+ we are continuously evaluating new architectures to see if there are some advantages compared to the currently popular ones+ and that will continue.,At the same level,More,More,The retirement of old hardware will allow us to take better advantage of vector/SIMD registers. Eventually if we reach a threshold+ we will request GPUs on Grid sites+ which will certainly boost our use. It is clear that many sites offer GPUs today+ though+ and we can take advantage of them already.,Yes,More expert software developers are _always_ useful. Physicists who understand software development at a reasonably deep level are also a rare commodity+ and we could use more of those. Those both tie to the issue of long-term contracts for people with such skill sets+ which are very rare.,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this,Practically we don't get contributions from outside the experiment. We have a mixture of super-expert software engineers and physicists offering part-time software support. As you'd expect+ the most knowledgeable folks tend to provide infrastructure+ and the least knowledgeable (in terms of software development) offer analysis tools.,Researchers dedicating a fraction of their time to software; Professional software engineers hired for this; Procured from external non-research entities,The majority of our computing tools are developed by professional software engineers+ though there are low-level contributions from physicists (or people trained as physicists+ at least). We procure a number of tools as well+ including Oracle for example (and of course use many open source or research-focused tools like IAM+ Globus+ Rucio+ PanDA+ ...).,Some do,No specific training,No,Basic programming (languages+ development tools+ ...); Advanced programming (languages+ development tools+ ...); Distribute computing (access to Grids+ storage systems+ AAI+ ...); Efficient programming (profiling+ energy measurement+ ...); Heterogeneous computing (GPUs+ FPGAs+ Quantum Computing+ ...),I mostly attended some training sessions offered in the field+ and had a few classes in high school. Our collaboration has an enormous array of people entering with a mixture of skills. We offer some training and take advantage of free training offered by others for some aspects. ,There is a generally big gap between the skills required for serious software development and the skills acquired via the standard training courses (which are introductory). People go many different places to acquire those skills; most learn on the job. Some regular mechanism for up-skilling workers would be valuable+ even to introduce new standards and best practices when they arise.,No,We try+ but have a great deal of difficulty offering sufficient recognition for these folks. They also don't get more permanent contracts at the rate that they should. Right now there is a burst of support for AI/ML jobs+ but those don't provide the same low-level software support that we need for our C++ and Python infrastructure; the AI/ML jobs tend to be data science type jobs of which we already have plenty.,,,,No,,,,,,,,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),ewe@ppp.com,,Experimental High Energy Physics (HEP),,fds,Individual,,fds,,,,,No (will skip most of the technical sections),,No,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
David Britton,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...); Manager of an e-Infractructure (for example a computing centre+ a storage facility+ a distributed computing facility),david.britton@glasgow.ac.uk,,Experimental High Energy Physics (HEP),,Lead UK HEP computing project GridPP and deputy lead of WLCG,Representing an initiative or a centre,GridPP,UK Grid Computing for HEP,,40,Yes,No,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management,Yes,,Experiment/Institution SSO; Federated (for example+ via eduGain); Username / Password,,username/password; macaroons; X509; 2FA; tokens,,No,,No,,Indigo-IAM; CERN-SSO,,,Model/theory calculations; Monte Carlo simulations; Offline data processing (calibration+ reconstruction+ …); High-level data analysis (visualization+ fitting+ …); Machine Learning/AI training/inference,,Batch (either directly submitted from a computing center+ or via some experiment / initiative middleware),,Single core; Multi/Many cores (on a single node),No,,WLCG pledge requirements in CRIC - UK fraction (around 12%) is also there.,Currently small,Currently almost zero,1 year,Pledges are renewed each year,Grid facilities,,,Fair share on batch systems; Opportunistic,,DIRAC; Openstack; HTCondor; Panda; glideinWMS; k8s,,,,Rely on GRID Provisioning,,No data needed; From local disks; From shared filesystems at the computing centre; Data downloaded programmatically before execution+ to local/centre disks; Data downloaded by the job itself or its wrapper before execution; Remote access via streaming,,Yes,,We support around 30 VOs; the answer will be different for different experiments.,,,,,,,,,,,Current pledges in CRIC are expected to be constrained by Flat Cash... so assume that they volume increases by 15% per year.,,See ATLAS and CMS HL-LHC requirements plots.,,At a single experimental site; At multiple computing centres (for example+ in case of simulations); At multiple experimental sites,LHC (real) data comes from single site; other data from sites worldwide (eg Belle and Fermilab).,,See LHCOPB abd LHCOne network diagrams,,We have many sites which have a range of connectivity. ,FTP; WebDAV; XrootD,,Yes+ via open software tools (e.g. Rucio+ DIRAC+ ...),,Multiple file / object replicas; Tiered storage systems (NVMe+ SSD+ HDD+ ...); Tape long term storage; Caches to optimize access time,,Yes (FTS),,Cluster Filesystems (GPFS+ EOS+ dCache+ Lustre+ ...),,Files,,,Varies and depends on VO,,Varies and depends on VO and job,,,Multiple replicas on disk; Multiple replicas on disk/tape; We accept a fraction of data loss; We rely on low level safety (RAID systems+ for example),,Needs an AAI mechanism (see the Compute Environment later),,,,,,,,,,,x86_64 (Intel+ AMD+ ...); aarch64,,Specific linux versions (please specify),,,Yes+ on distributed file systems (for example+ CVMFS),No+ but the node needs to be able to open connections to exrternal nodes (for example+ via a NAT mechanism),1 core; Multicore jobs on a single node (not using the while node),Nvidia GPUs; AMD GPUs; Intel GPUs,1,Some workflows from some clients can offload significant fraction to GPUs+ but most can offload none.,I will not login+ I use an (experiment/public/whatever) service which gets the resource for me (e.g. Rucio+ DIRAC+ Dask+ ...),,Via CVMFS,,Singularity / Apptainer; Docker,,,Depends on whether I am a user or a provider,,Depends,Yes+ we need services executing on special nodes,Depends on VO but we certainly need accounting and monitoring services,,,Increased use ar aarch64; gradual increase in GPU use;,Evolution but not revolution.,No,Possible whole-node ,Python; C/C++,Minor use of Fortran; some interest in Julia; ,,Python ecosystem (NumPy+ scikit-learn+ …); Hardware oriented libraries (CUDA+ sycl+ opencl+ openMPI+ alpaka+ …); AI toolkits / frameworks (keras+ pytorch+ tensorflow+ …); Simulation toolkits (geant+ fluka+ corsika+ …); External generators or general physics tools (pythia+ madgraph+ sherpa+ fastjet+ hepmc ..); HEP specific software (ROOT+ dd4hep+...); Generic C++ libraries and compilers (boost+ gcc+ llvm+ cling+ clang),,,x86_64; riscv64; aarch64,,,,,Cost/performance optimization,Energy Efficiency is also a factor,,,,,,,,,More,More,More,,No,People,Yes+ since the budget for computing resources will not be sufficient simply extrapolating current software; Yes+ because data will become more complex; Yes+ because data volumes will increase,,,,,,,,,,,,,,,,,,,,,,,,,Cloud (CPU only); Cloud (CPU and GPU); HTC CPU only (Grid-style+ batch+ ...); HTC CPU+GPU (Grid-style+ batch+ ...); Storage systems (disks); Storage systems (tapes),,It is a WLCG Tier0/1/2/3,,Other (please specify),90% is for LHC and 10% supports the rest of the UK STFC PPAN programme (Particle Physics+ Astro and Nuclear),,,,,,,,We are about 12% of WLCG. We measure CPU in HS23 (not cores). The numbers from the accounting portal are the best estimate. We don't have aggregated numbers on Power+ Surface+ or Racks.,,Access is via the Grid,,Grid access but some VO boxes,Via batch system (SLURM+ LSF+ PBS+ HTCondor+ ...); Via Cloud interfaces (k8s+ Openstack+ ...); Static allocations,,Batch queues (for example SLURM+ HTCondor+ PBS+ LSF+ …); Via a Cloud Middleware / Stack; Via Kubernetes,,,,,x84_64; aarch64,,,,> 64,,2-4,,No,,,,Standard ethernet but with many sites I can't generalise an answer.,,> 100 Gbps,We use LHCOPN at RAL and LHCONE and RAL and Imperial (and other sites eventually),,,,,,Yes+ via internally developed tools (please specify in the next question),Mostly Rucio,,GPFS; Xrootd; Lustre; dCache; EOS,CTA,,POSIX (for example+ NSF mount or any other fuse mount); WebDAV; Xrootd,,WebDAV; Xrootd,,,,CVMFS; Virtualization (singularity+ apptainer+ docker+ udocker+ shifter+ ..),,Yes,,Operational choices (node shutdown when not needed+ lower clock+ job packing to maximise systems which can be shut down+ ...),Can't generalise over a dozen sites,Yes+ we are doing it (please specify below)
Tommm,Researcher / user of scientific computing resources (doing analysis+ R&D+ operations+ ...),t5omm@ooo.com,,Experimental High Energy Physics (HEP),,fff,Representing an initiative or a centre,fdfsd,fdsfsd,,,Yes,Yes,Yes,Authentication and Authorization; Workload Management; Data Management; Computing Environment; Software+ software development and software management; Training and careers,Yes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
